---
title: "Predicting Breast Tumor Malignancy Using Generalized Additive Models"
author: "Laura Cai, Ava Exelbirt, Abby Li, Grady Purcell, Ella Tillinghast"
date: "`r Sys.Date()`"
editor_options:
  markdown:
    wrap: 72
format:
  pdf:
    cite-method: biblatex
    link-citations: true
    include-in-header:
        text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
         \DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
    header-includes:
      - \usepackage{indentfirst}
      - \setlength{\parindent}{1.5em}
      - \setlength{\parskip}{0pt}
      - \setlength{\textfloatsep}{6pt}      % space below floats
      - \setlength{\floatsep}{6pt}          % space between floats
      - \setlength{\intextsep}{6pt}         % space around tables/figures
      - \setlength{\abovecaptionskip}{2pt}  % space above caption
      - \setlength{\belowcaptionskip}{2pt}  % space below caption
bibliography: references.bib
link-citations: true
geometry: margin=0.5in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,        # hide code by default
  message = FALSE,     # hide messages
  warning = FALSE,     # hide warnings
  fig.align = "center"
)

```

# **Introduction**

## Project Goals

The primary goal of our project is to build an interpretable machine
learning model that can assess whether a breast mass is benign or
malignant using measurable characteristics such as radius, area,
smoothness, and concavity extracted from digitized medical images. Our
analysis has two complementary aims: 1. prediction, where we train a
classifier to estimate the probability that a new mass is malignant, and
2. inference, where we identify and interpret the features that have the
greatest influence on these probability assignments. Because our target
audience includes clinicians, researchers, and individuals without a
technical background, we emphasize transparency and interpretability,
focusing not only on whether the model predicts accurately, but also on
why it arrives at its predictions.

The project also addresses several relevant challenges, including
potential noise in imaging, derived features, limitations associated
with an older dataset, and the need for models that doctors can trust
when making time-sensitive decisions. Trust and interpretability are
central concerns in medical applications, and our modeling pipeline is
intentionally designed to balance performance with transparency,
ensuring that the results are clinically meaningful and accessible.

Our code is available at https://github.com/gradypurcell/STA-325-Final-Project/tree/main [@github].

## Importance and Relevance

Breast cancer is one of the most pressing health challenges worldwide
and remains the second most common cancer among women, accounting for
roughly 30% of all new female cancer cases each year, and the second
leading cause of cancer related deaths among women [@acs_facts]. As incidence rates continue to rise by approximately 1%
annually, improving tools for early differentiation between benign and
malignant tumors is critical for timely intervention. Early detection of
breast cancer has been shown to boost survival rates by up to 20%,
further underscoring the clinical relevance of high quality diagnostic
support tools [@pubmed].

Traditional diagnostic techniques like mammography, ultrasound, MRI, and
biopsy are essential but can also be expensive, time consuming, and
prone to human error, particularly false positives and false negatives.
Recent research demonstrates that machine learning (ML) can meaningfully
strengthen diagnostic processes. For instance, ML models have shown
higher accuracy than clinicians in predicting several cancers, including
breast, brain, and lung cancer, where 2020 DeepMind-based
system outperformed human specialists in breast cancer detection, and other ML systems have reached 97% accuracy in identifying common types of lung cancer [@pmc;@nature]. These findings highlight the potential for ML driven tools to enhance clinical
workflows and reduce diagnostic uncertainty.

Even within the specific Wisconsin Breast Cancer dataset we analyzed,
prior research has achieved strong performance: Support Vector Machines
have reached 87–89% accuracy in distinguishing malignant from benign
masses, and deep learning models such as CNNs have successfully
extracted subtle features (e.g., microcalcifications or architectural
distortions) that radiologists may overlook [@pubmed].

Although the dataset we analyze is more than two decades old, our
project serves as a proof of concept illustrating how interpretable ML
methods can support clinical reasoning. Because modern imaging systems
continue to quantify many of the same structural features used in this
dataset, our modeling framework remains relevant, scalable, and
adaptable. By identifying the features most strongly associated with
malignancy, we provide insights that can inform research, guide
clinicians’ early decision making, and support the development of future
diagnostic tools designed to complement, not replace, medical expertise.

## Overview of Proposed Methodology

Since our model would assist doctors with diagnosing cancer patients, our model needs to be interpretable so that doctors are more confident in their diagnosis and can explain the factors which led to the final diagnosis. To do this, we will first use a LASSO logistic regression model to shrink the features so that only relevant features are included in the final model-- there is likely multicollinearity between many of the features that shrinkage addresses. Once the LASSO logistic model provides the relevant features, we will build a Generalized Additive Model (GAM) since our EDA has indicated nonlinearity between the predictors and state of the tumor, so we want our final model to have a balance between flexibility and interpretability. After selecting which features should be included as non-linear splines in the model, we include interactions based on exploratory analysis, domain knowledge, and model performance. 

# **Data**

The dataset used in this analysis comes from the Diagnostic Wisconsin
Breast Cancer Database (WBCD), originally created from digitized images
of fine needle aspiration (FNA) biopsies taken from breast masses [@data]. The
version used here was downloaded from Kaggle, but the source dataset was
donated on October 31, 1995. Although imaging modalities have improved
significantly since 1995, modern systems continue to quantify similar
structural and morphological features, meaning that this dataset remains
an informative foundation for building a proof-of-concept modeling
framework that can later be extended to more advanced imaging
technologies.

Each observation corresponds to a single breast mass and contains 30
continuous predictors, derived from 10 underlying cell nucleus features:
radius, texture, perimeter, area, smoothness, compactness, concavity,
concave points, symmetry, fractal dimension. Each of these ten features
was recorded in three forms: 1.  Mean – the average value across the tumor sample. 2.  Standard Error (SE) – the variability of that feature. 3.  Worst (maximal) – the mean of the highest three values

The response variable is the diagnosis: malignant (M) or benign (B). There are 569 observations, including 357 benign (62.7%) and 212
malignant (37.2%) cases. Although benign tumors are more common in the dataset, 
the dataset is only moderately imbalanced, so rebalancing procedures such as
SMOTE or class weighting are not necessary for this analysis.

The data contains no missing values, and all predictors were
standardized with four significant digits in the version we downloaded.
Because the features are continuous, structured, and well behaved, this
dataset is well suited for multiple modeling approaches including
logistic regression, LASSO, KNN, random forests, and decision trees.

Finally, this dataset provides a solid platform for exploratory data
analysis (EDA) and model interpretation because each predictor has a
clear biological meaning and can be reasoned about in clinical terms
(e.g., radius relates to tumor size, concavity relates to irregularity).

The 13 predictors selected from the LASSO logistic model can be grouped into three categories: those which measure mean, standard error, and worst. The mean variables include compactness and concave points. The standard error features are radius, texture, smoothness, compactness and fractal dimension. Lastly, the worst variables are radius, texture, smoothness, concavity, concave points and symmetry. @tbl-key-vars describes these the key variables. 

```{r, label = "tbl-key-vars"}
library(kableExtra)
variables <- data.frame(
  Variable = c(
  "Radius", "Texture", "Smoothness", "Fractal dimension",
  "Symmetry", "Compactness", "Concave points", "Concavity"
  ),
  Description = c(
  "Mean of distances from center to points on perimeter",
  "Standard deviation of gray-scale values",
  "Local variation in radius lengths",
  "“coastline approximation” − 1",
  "Symmetry of cell nuclei",
  "perimeter² / area − 1.0",
  "Number of concave portions on contour",
  "Severity of concave portions of the contour"
  )
)
kable(variables, caption = "Key Feature Variable Descriptions", align = "c") |>
  kable_styling(latex_options = c("hold_position", "scale_down"),
                font_size = 9,
                full_width = FALSE)
```

# **Modeling Methodology**

Our methodology consists of five main stages: EDA, threshold optimization and evaluation, preliminary modelcomparison, regularized model selection, and nonlinear modeling with GAMs.

## Exploratory Data Analysis (EDA)

```{r, eval=TRUE}
# Loading the packages
library(tidyverse)
library(ggplot2)
library(dplyr)
library(class)
library(caret)
library(pROC)
library(glmnet)
library(GGally)
library(mgcv)
library(tidyr)
library(knitr)
library(kableExtra)
library(patchwork)
```

We began by conducting a comprehensive exploratory data analysis to
understand the distribution of each of the 30 predictors and to identify
any necessary transformations or outlier adjustments. Visualizations
included histograms, boxplots, and scatterplots across both classes
(benign vs. malignant).

Although certain variables showed skewed distributions, which is
expected in biological measurements, none raised any concerns severe enough
to justify transformation, especially given that GAMs do not need the linear assumptions. Outliers were examined but ultimately retained, 
as they reflect real biological variation rather than measurement error. 
No missing data were present, so no imputation or deletion procedures were 
required. The EDA thus served primarily to guide expectations about predictor 
behavior and to inform our choice of appropriate models.

```{r, results = 'asis', fig.cap="EDA for Interaction Terms", out.width="100%", fig.width=12, fig.height=4, label="fig-eda-interactions"}
df <- read.csv("Data/data.csv")

# smoothness_worst vs concavity_worst
p1 <- ggplot(df, aes(smoothness_worst, concavity_worst, color = diagnosis)) +
  geom_point() +
  geom_smooth(method = "gam") + 
  ggtitle("Smoothness (Worst) \nand Concavity (Worst)") + 
  theme_bw()

# radius_worst vs concave.points_mean
p2 <- ggplot(df, aes(radius_worst, concave.points_mean, color = diagnosis)) +
  geom_point() +
  geom_smooth(method = "gam") + 
  ggtitle("Radius (Worst) \nand Concave Points (Worst)") + 
  theme_bw()

# fractal_dimension_se vs concavity_worst
p3 <- ggplot(df, aes(fractal_dimension_se, concavity_worst, color = diagnosis))+
  geom_point() +
  geom_smooth(method = "gam") + 
  ggtitle("Fractal Dimension (SE) \nand Concavity (Worst)") + 
  theme_bw()


(p1 | p2 | p3) + plot_layout(guides = "collect")
```

We also explored bivariate EDA to examine potential interaction effects between predictors. In @fig-eda-interactions, there appears to be interaction effects between each of the listed pairs of predictors since the relationships between each pair differs with the benign and malignant tumors.

## Building a Model of a Mass

For two observations of a malignant and a benign tumor, we have attempted to translate the variables of the model into a representation of a mass to gain intuition into the features of the mass and explore the differences between malignant and benign tumors. 

```{r, label = "fig-mass", fig.width=4, fig.height=3, fig.cap = "Simulated Tumor Cross-Section: Benign vs Malignant"}

# pick one malignant and one benign case
malignant_tumor <- df %>%
  filter(diagnosis == "M") %>%    # 1 = malignant
  slice(1)

benign_tumor <- df %>%
  filter(diagnosis == "B") %>%    # 0 = benign
  slice(1)

# malignant_tumor
# benign_tumor

simulate_tumor <- function(radius_mean, concavity_worst,
                           n_points = 6000, seed = 1) {
  set.seed(seed)
  phi   <- runif(n_points, 0, pi)
  theta <- runif(n_points, 0, 2*pi)

  base_r  <- radius_mean
  bump_sd <- concavity_worst * 3  # roughness control

  r <- pmax(0.1, rnorm(n_points, mean = base_r, sd = bump_sd))

  x <- r * sin(phi) * cos(theta)
  y <- r * sin(phi) * sin(theta)
  z <- r * cos(phi)

  tibble(x = x, y = y, z = z, r = r)
}

tumor_mal <- simulate_tumor(malignant_tumor$radius_mean,
                           malignant_tumor$concavity_worst,
                           seed = 10) %>%
  mutate(type = "Malignant (1)")

tumor_ben <- simulate_tumor(benign_tumor$radius_mean,
                           benign_tumor$concavity_worst,
                           seed = 20) %>%
  mutate(type = "Benign (0)")

tumors <- bind_rows(tumor_mal, tumor_ben)

lim <- max(abs(tumors$x), abs(tumors$y))

ggplot(tumors, aes(x = x, y = y)) +
  geom_point(aes(color = type), alpha = 0.35, size = 0.35) +
  facet_wrap(~type, nrow = 1) +
  coord_equal(xlim = c(-lim, lim), ylim = c(-lim, lim)) +
  labs(
    x = "X (mm)", y = "Y (mm)", color = NULL
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none", plot.title = element_text(size = 10))
```
Based on the tumor cross sections in @fig-mass the selected malignant tumor appears to be larger and more irregular and dispersed around the edges compared to the selected benign tumor. Thus, the observed malignant tumor has a larger average radius and higher concavity measures than the benign tumor. 

Based on initial observations, the summary statistics show a consistent pattern, indicating that malignant tumors (diagnosis = 1) tend to have higher average sizes, due to higher average radius values, and greater irregularity, due to high average concavity values, compared to benign tumors (diagnosis = 0), as seen in @tbl-benign-malignant.

```{r, label = "tbl-benign-malignant"}
summary_stats <- df %>%
  mutate(
    diagnosis_num =
      if (is.numeric(diagnosis) || is.integer(diagnosis)) diagnosis
      else ifelse(diagnosis == "M", 1, 0)
  ) %>%
  group_by(diagnosis_num) %>%
  summarise(
    mean_radius_mean  = mean(radius_mean, na.rm = TRUE),
    mean_radius_worst = mean(radius_worst, na.rm = TRUE),
  
    mean_concavity_mean  = mean(concavity_mean, na.rm = TRUE),
    mean_concavity_worst = mean(concavity_worst, na.rm = TRUE)
  )
colnames(summary_stats) <- c("Diagnosis", "Avg. Mean Radius", "Avg. WorstRadius", "Avg. Mean Concavity", "Avg. Worst Concavity")
kable(
  summary_stats,
  digits = 2,
  caption = "Benign vs Malignant Tumor Characteristics",
  align = "c"
) |>
  kable_styling(latex_options = c("hold_position", "scale_down"),
                font_size = 9,
                full_width = FALSE)
```

## Cost Analysis and Threshold Selection 

We determine the optimal classification threshold that reflect the asymmetric cost of false negatives (missing a malignant tumor) versus false positives
(unnecessary further testing). Missing a malignant tumor has substantially higher medical, emotional, and financial costs. Early diagnosis improves survival and enables more effective treatment, and clinicians consistently emphasize the psychological and clinical urgency of the timely detection [@utmd]. 

It is important to understand that each breast-cancer-related death results in the loss of a mother, sister, or partner. The impact cancer emotionally has on an individual household cannot be quantified but should not go unacknowledged. As we cannot provide a summary statistic on grief, healthcare is a field where we are touching the lives of real people. Moreover, we choose to craft our model in a way that reflects our consideration of protecting general livelihood, not just for the patient, but for those who love her as well.

Early-stage treatment is far more cost-efficient. For example, women treated when the tumor is small and localized pay \$48,500 on average compared to over \$183,00* for stageIV disease [@webmd_costs]. We prioritize this concern as 1 in 13 women report cancer treatment costs are higher than intitially anticipated, so much so that women also report avoiding going to the doctor in fear of having to pay an exuberant amount. National analyses similarly estimate that early cancer detection could save $26 billion per year nationally, due to reduced treatment intensity, and improved outcomes [@Brill2020]. Thus, the economic and clinical disparities underscore the importance in prioritizing sensitivity to minimize false negatives, diagnosing the tumor as soon as possible.

Prior research also shows that the accuracy of mammography screening increases with patient age, with sensitivity ranging from 76% to 86% and specificity ranging from 87% to 99% across age groups (@Newton2025). This gap in threshold shows why maximizing sensitivity matters: since detecting malignant tumors is the harder task, optimizing sensitivity is essential to improving screening effectiveness. Thus, our decision to choose a lower threshold is consistent with addressing current screening practices and reducing future costs associated with delayed diagnoses. 



## Preliminary Model Fitting

We fit several classification models to evaluate baseline
predictive performance and to assess the trade-offs between accuracy,
interpretability, computational complexity, and clinical
trustworthiness. The models included: K-Nearest Neighbors (KNN), Random
Forest, LASSO, Standard Logistic Regression, and Decision Trees. We evaluate each model using standard metrics including accuracy,
sensitivity, specificity, and confusion matrices and use cross validation where appropriate to reduce sampling variability.

Although the random forest achieves strong predictive accuracy, it
lacks interpretability, which should be prioritized since this model will be used by doctors to diagnose whether a breast cancer mass is benign or malignant at an early stage. 
KNN performs reasonably but does not provide coefficients, feature weights, or insight into mechanism. Standard logistic regression is interpretable but tends to overfit
without regularization due to the high number of correlated predictors.

## Regularized Logistic Regression and Feature Selection
Based on accuracy, interpretability, stability, and feature selection
capability, we selected the LASSO logistic regression model as our
primary predictive and inferential tool, since it performs
variable selection,
providing a concise, clinically meaningful set of predictors and
potential interactions. The diagnosis variable was
encoded as 1 for malignant and 0 for benign. Identifier columns were
removed, and the remaining predictors are standardized. The data was
randomly split into training (80%) and testing (20%) sets. A LASSO
logistic regression model was fit using 5-fold cross-validation to
select the optimal penalty parameter with `cv.glmnet`. The final LASSO logistic model
achieves approximately 99% accuracy on the held-out test set, with
perfect sensitivity and strong specificity. 

### Setting the Optimal Threshold
We determined the optimal threshold to predict whether a mass is benign or malignant based on the assigned probability. From the cost analysis and domain knowledge, we knew that the optimal threshold would maximize sensitivity. Using the `proc` library, we found that the optimal threshold to achieve perfect sensitivity is 0.351 using Youden's J statistic. Youden's method is a way to choose the "best" classification threshold by maximizing the combined performance of sensitiivty and specificity. The optimal threshold is one that maximizes J = Sensitivity + Specificity - 1. For this diagnostic classification problem, Youden's J statistic chooses the threshold that provides a balance weight to correctly identifying malignant and benign cases. Since cancer diagnosis requires higher sensitivity to avoid missing malignant cases, we want the threshold to be less than 0.5, so 0.351 seems a reasonable value. Other benefits for using Youden's statistics is that it is not distorted by how common cancer is in the dataset and it is transparet for clinicians. Although perfect sensitivity often comes at the cost of poor specificity and many false positives, we also observe a high specificity of 0.98 with perfect sensitivity, so we used 0.351 as the threshold for classifying a tumor.

```{r}
df <- read.csv("Data/data.csv")
df <- df |> select(-X)
df <- data.frame(df)

# Adding a categorical variable for diagnosis
df$diagnosis <- ifelse(df$diagnosis == "M", 1, 0)
df <- df[ , !names(df) %in% c("id", "X", "Unnamed..32") ] # remove id columns

# Preparing the data for the train/test split
X <- model.matrix( ~ ., data = df)[, -1]
y <- df$diagnosis

# Train/test split
set.seed(123) 
n <- nrow(X)
train_idx <- sample(seq_len(n), size = 0.8 * n)

X_train <- X[train_idx, ][, -1]
X_test  <- X[-train_idx, ][, -1]
y_train <- y[train_idx]
y_test  <- y[-train_idx]

# Building the LASSO model
cvfit <- cv.glmnet(
  x = X_train,
  y = y_train,
  family = "binomial",
  alpha = 1,        # LASSO
  nfolds = 5        # 5-fold CV
)

# Predicted probabilities (logistic) at lambda.min
prob_test <- predict(cvfit, newx = X_test, 
                     s = "lambda.min", 
                     type = "response")
prob_test <- as.numeric(prob_test)

roc_obj <- roc(response = y_test, predictor = prob_test, positive = "1")

# Extracting the optimal threshold
coords <- coords(roc_obj,
       x = 1,
       input = "sensitivity",
       ret = c("threshold", "sensitivity", "specificity"))
opt_threshold <- coords$threshold

# Predicted class based on the optimal threshold
pred_class <- ifelse(prob_test >= opt_threshold, 1, 0)
```

```{r, eval=TRUE, results = 'asis', tbl.cap="Confusion Matrix with LASSO", out.width="80%", label = "tbl-conf-mat"}
# Building the Confusion Matrix
cm <- table(Predicted = pred_class, Actual = y_test)
cm_df <- as.data.frame.matrix(cm)
cm_labeled <- data.frame(
  Outcome = c("Predicted Benign", "Predicted Malignant"),
  Benign  = cm[, "0"],
  Malignant = cm[, "1"]
)
kable(cm_labeled, caption = "Confusion Matrix", align = "c") |>
  kable_styling(latex_options = c("hold_position", "scale_down"),
                font_size = 9,
                full_width = FALSE)
```
```{r, eval=FALSE}
accuracy <- mean(pred_class == y_test)
sens <- sum(pred_class == 1 & y_test == 1) / sum(y_test == 1)
spec <- sum(pred_class == 0 & y_test == 0) / sum(y_test == 0)
c(accuracy = accuracy, sensitivity = sens, specificity = spec)
```

### Model Diagnostics 
The Confusion Matrix above uses the optimal threshold and achieves perfect sensitivity, a specificity of 0.98, and a 99% accuracy. This indicates strong overall model performance and although there are possible concerns about model overfitting, we are confident in our results given that we used 5-fold CV to train the data and then tested the data on the hold-out set.  
```{r}
accuracy <- mean(pred_class == y_test)
sens <- sum(pred_class == 1 & y_test == 1) / sum(y_test == 1)
spec <- sum(pred_class == 0 & y_test == 0) / sum(y_test == 0)
#c(accuracy = accuracy, sensitivity = sens, specificity = spec)
```

Model performance was further evaluated using an ROC curve and a measure of AUC, providing a threshold-independent measure of discrimination. The AUC for the ROC curve is 0.9994 which is fairly high, giving us confidence in our model. 

The last step after building the LASSO logistic model was extracting the features which had non-zero coefficients for predicting the probability of a tumor being benign or malignant. @tbl-selected-features shows the 13 features (out of 30) which the LASSO model indicated were important for predicting the probabilities ordered in descending order of magnitude. 
```{r, eval=TRUE, label= "tbl-selected-features"}
# Extracting the coefficient features
coef_min <- coef(cvfit, s = "lambda.min")
coef_df <- as.data.frame(as.matrix(coef_min))
coef_df$feature <- rownames(coef_df)
names(coef_df)[1] <- "coefficient"

# Filtering for coefficients which aren't 0
feature_list <- coef_df |>
  filter(feature != "(Intercept)") |>
  filter(coefficient != 0) |>
  arrange(desc(abs(coefficient))) |>
  pull(feature)

# Grouping by the values they end in
group_se    <- feature_list[grepl("_se$", feature_list)]
group_mean  <- feature_list[grepl("_mean$", feature_list)]
group_worst <- feature_list[grepl("_worst$", feature_list)]

# Determine longest group
max_len <- max(length(group_se), length(group_mean), length(group_worst))

# Pad groups to same length
group_se_padded    <- c(group_se,    rep("", max_len - length(group_se)))
group_mean_padded  <- c(group_mean,  rep("", max_len - length(group_mean)))
group_worst_padded <- c(group_worst, rep("", max_len - length(group_worst)))

# Build final data frame
df_grouped <- data.frame(
  Mean  = group_mean_padded,
  SE    = group_se_padded,
  Worst = group_worst_padded
)

kable(
  df_grouped,
  caption = "Selected Features",
  align = "c"
) |>
  kable_styling(latex_options = c("hold_position", "scale_down"),
                font_size = 9,
                full_width = FALSE)
```

In summary, the LASSO logistic model yielded a parsimonious subset of clinically
interpretable predictors, which served two purposes:

1.  Establishing a transparent, high-performing baseline classifier.
2.  Guiding subsequent nonlinear modeling by identifying variables most
    strongly associated with malignancy.

## Nonlinearity Assessment and Motivation for GAMs

While LASSO logistic regression provides interpretability through
coefficients, it assumes linear relationships between predictors and the
log-odds of malignancy. To assess whether this assumption was
appropriate, we conducted a systematic investigation of potential
nonlinearity.

For each predictor selected by the LASSO model, we binned the predictor
into quantiles, computed observed malignancy proportions within each
bin, and examined both probability-scale and log-odds-scale plots.
Several predictors, most notably texture_se, compactness_se,
fractal_dimension_se, texture_worst and concavity_worst, exhibited clear nonlinear patterns,
suggesting that a purely linear model may be misspecified.

## Generalized Additive Models (GAMs)
We fit GAMs informed by the LASSO selected features as it provides interpretability and flexibility in including nonlinear terms. GAM models are more interpretable than many other regression techniques since they enable visualization between features and the predicted malignancy probability with interpretability being essential for clinical decisions. GAMs model the log-odds of malignancy as a sum of smooth functions of predictors, allowing nonlinear relationships to be captured without requiring explicit interaction terms.


We compared different spline bases (thin-plate regression splines,
cubic regression splines, and thin-plate shrinkage splines) for individual predictors using AIC, REML
scores, and visual inspection. Restricted Maximum Likelihood (REML) was used to compare the splines instead of Maximum Likelihood because REML provides less biased estimates for variance components. Although no spline performed definitively better across all metrics, cubic splines generally had lower AIC and REML across variables. Additionally, cubic splines had a lower range of plausible predicted probabilities and less wiggly than the other two splines. Thus, we chose to use cubic regression splines since consistently
provided better stability and more realistic probability behavior, and
were therefore selected.

We then selected which features show strong violation of nonlinearity so we can include them in splines. All other variables which don't show strong evidence of nonlinearity will be included as main effects or interaction terms, so we first binned each of the predictors and calculated the appropriate log-odds of malignancy within the training data. We then visualized the logit plot for each predictor to determine where we could see strong violations of nonlinearity. 

```{r}
# Getting the training dataframe
X2 <- model.matrix( ~ ., data = df)[, -1]

# Getting the dataframes
train_df <- as.data.frame(X2[train_idx, ])
test_df <- as.data.frame(X2[-train_idx, ])
```

```{r, eval = TRUE, results = 'asis', fig.cap="Log-Odds Plots", out.width="80%", fig.width=12, fig.heiught=12, label = "fig-log-odds"}
nbins <- 20
yvar <- "diagnosis"       # your binary outcome

# Function that bins & plots one variable
make_binned_plots <- function(df, xvar, yvar, nbins = 20) {
  
  # 1) Compute bins
  binned <- df %>%
    mutate(bin = ntile(.data[[xvar]], nbins)) %>%
    group_by(bin) %>%
    summarise(
      x_mid = mean(.data[[xvar]], na.rm = TRUE),
      n     = n(),
      prop  = mean(.data[[yvar]], na.rm = TRUE),
      .groups = "drop"
    ) %>%
    mutate(
      se    = sqrt(prop * (1 - prop) / pmax(n, 1)),
      lower = pmax(0, prop - 1.96 * se),
      upper = pmin(1, prop + 1.96 * se),
      eps   = 1e-3,
      logit = log((prop + eps) / (1 - prop + eps))
    )
  
  # 2) Proportion plot
  p_prop <- ggplot(binned, aes(x = x_mid, y = prop)) +
    geom_point(size = 2) +
    geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.01) +
    labs(
      title = paste("Binned proportion for", xvar),
      x = xvar,
      y = "Observed proportion"
    ) +
    theme_minimal()
  
  # 3) Logit plot
  p_logit <- ggplot(binned, aes(x = x_mid, y = logit)) +
    geom_point(size = 2) +
    geom_smooth(method = "loess", se = TRUE) +
    labs(,
      x = xvar,
      y = "Estimated log-odds"
    ) +
    theme_minimal()
  
  return(list(binned = binned, prop_plot = p_prop, logit_plot = p_logit))
}

# ---- Loop over all predictors ----
results <- map(feature_list, ~ make_binned_plots(train_df, .x, yvar))
names(results) <- feature_list

## Showing all of the plots
p1 <- results[["smoothness_se"]]$logit_plot
p2 <- results[["concave.points_mean"]]$logit_plot
p3 <- results[["radius_se"]]$logit_plot
p4 <- results[["radius_worst"]]$logit_plot
p5 <- results[["texture_worst"]]$logit_plot
p6 <- results[["smoothness_worst"]]$logit_plot
p7 <- results[["concavity_worst"]]$logit_plot
p8 <- results[["concave.points_worst"]]$logit_plot
p9 <- results[["symmetry_worst"]]$logit_plot
p10 <- results[["compactness_mean"]]$logit_plot
p11 <- results[["texture_se"]]$logit_plot
p12 <- results[["compactness_se"]]$logit_plot
p13 <- results[["fractal_dimension_se"]]$logit_plot

# Arranging the plots
plots <- list(
  p1, p2, p3, p4, p5, p6,
  p7, p8, p9, p10, p11, p12, p13
)

wrap_plots(plots, ncol = 3) & 
  theme(legend.position = "bottom")

```

In @fig-log-odds, we observe that many of the variables to violate nonlinearity. Thus, we also want to quantitatively determine which features should be included as splines by comparing a GLM model to predict malignancy probability with a GAM using only a cubic spline with the selected feature. Then, we compared the change in AIC between the GLM and GAM as well as the p-value for the Chi-Squared test where the cutoff was set at $\alpha = 0.01$. Features which had a drop in AIC with a magnitude of at least 4 and a significant p-value were identified as features with strong evidence of non-linearity. The delta AIC value of 4 was selected using the convention that a drop in AIC greater than 4 is evidence that the GAM meaningfully improves the model fit. 

```{r, eval=TRUE, results = 'asis', tbl.cap="Features with Strong Nonlinearity", out.width="80%", label = "tbl-nonlin-feat"}
results <- map_dfr(feature_list, function(var) {
  
  # build formulas
  form_glm <- as.formula(paste("diagnosis ~", var))
  form_gam <- as.formula(paste("diagnosis ~ s(", var, ", k=10)", sep = ""))
  
  # fit models
  mod_glm <- glm(form_glm, data=train_df, family = binomial("logit"))
  mod_gam <- gam(form_gam, data=train_df, bs="cr",
                 family = binomial("logit"), method="REML")
  
  # AIC comparison
  aic_vals <- AIC(mod_glm, mod_gam)
  
  # Likelihood ratio test (nested model test)
  an <- anova(mod_glm, mod_gam, test="Chisq")
  pval <- an$`Pr(>Chi)`[2]   # GAM vs GLM

  
  tibble(
    variable = var,
    glm_AIC  = aic_vals$AIC[1],
    gam_AIC  = aic_vals$AIC[2],
    delta_AIC = aic_vals$AIC[1] - aic_vals$AIC[2],
    LRT_pvalue = an$`Pr(>Chi)`[2],  # the p-value comparing GLM vs GAM
    significant_0.01 = ifelse(!is.na(pval) & pval < 0.01, "YES", "NO")

  )
})
results |> 
  filter(abs(delta_AIC) > 4, significant_0.01 == "YES") |>
  select(variable, delta_AIC) |>
  kable(caption = "Features with Strong Nonlinearity") |>
    kable_styling(latex_options = c("hold_position", "scale_down"),
                  font_size = 9,
                  full_width = FALSE)

```

From @tbl-nonlin-feat, the following features were included in the GAM as cubic splines: texture_se, compactness_se, fractal_dimension_se, texture_worst, and concavity_worst. 

From here, the initial GAM model was built using cubic splines for 5 nonlinear features and including the other 8 features as main effects. 
```{r, eval=TRUE, results = 'asis', out.width="80%", label = "tbl-conf-mat-no-inter"}
formula_all <- diagnosis ~ 
  s(texture_se, bs = "cr", k = 10) +
  s(compactness_se, bs = "cr", k = 10) +
  s(fractal_dimension_se, bs = "cr", k = 10) +
  concave.points_mean + radius_se + radius_worst +
  s(texture_worst, bs = "cr", k = 10) +
  s(concavity_worst, bs = "cr", k = 10) + 
  compactness_mean  + concave.points_mean + radius_se + 
  smoothness_se + radius_worst + smoothness_worst + concave.points_worst + 
  symmetry_worst

mod_all <- bam(formula_all,
               data = train_df,
               family = binomial(link = "logit"),
               method = "fREML",
               discrete = TRUE,      # huge speed boost
               select = TRUE)

df_test_pred <- test_df |>
  mutate(
    .pred_prob  = predict(mod_all, newdata = test_df, type = "response"),
    .pred_link  = predict(mod_all, newdata = test_df, type = "link"),
    .pred_class = as.integer(.pred_prob >= opt_threshold),
    .obs        = as.integer(!!rlang::sym("diagnosis"))
  )

cm <- caret::confusionMatrix(
  factor(df_test_pred$.pred_class, levels = c(0,1)),
  factor(df_test_pred$.obs, levels = c(0,1)),
  positive = "1"
)

#cm$overall["Accuracy"]
#cm$byClass[c("Sensitivity", "Specificity")]

cm2 <- table(Predicted = df_test_pred$.pred_class, Actual = df_test_pred$.obs)
cm_df2 <- as.data.frame.matrix(cm2)
cm_labeled2 <- data.frame(
  Outcome = c("Predicted Benign", "Predicted Malignant"),
  Benign  = cm2[, "0"],
  Malignant = cm2[, "1"]
)
kable(cm_labeled2, caption = "Confusion Matrix for Model without Interactions", align = "c") |>
  kable_styling(latex_options = c("hold_position", "scale_down"),
                font_size = 9,
                full_width = FALSE)

roc_obj <- roc(df_test_pred$.obs, df_test_pred$.pred_prob, quiet = TRUE)
auc_val <- as.numeric(pROC::auc(roc_obj))
#print(auc_val)
```

The model performance for this base GAM was assessed using the test dataset. The model has a 98% accuracy as well as a sensitivity of 0.981 and a specificity of 0.983. The model also has an area under the curve extremely close to 1 indicating the model has a strong ability to distinguish between classes. For cancer diagnoses we want to maximize sensitivity, so we next consider adding interactions to improve our model to further increase sensitivity. 

```{r, eval=FALSE}
summary(mod_all)
```

Based on the output to our GAM model without interactions, there are several linear and nonlinear predictors that have statistically significant relationships with the log odds of diagnosis. For linear predictors, it appears that the mean concave points, radius SE, and radius worst are positively associated with tumor malignancy, such that larger tumors and those with more concave points increase the log-odds them being malignant. Furthermore, mean compactness shows a negative association with tumor malignancy, indicating that on average, the more compact a tumor is, the less likely it is to be malignant. For nonlinear predictors, it appears that texture SE and texture worst have a statistically significant non-linear relationship with tumor malignancy, indicating that variability in tumor texture does impact diagnosis. 

## Adding Interaction Effects

Although GAMs capture nonlinear marginal effects, additive models may
miss important interactions. Guided by domain knowledge and prior
findings on the Wisconsin Breast Cancer Dataset, we extended the
additive GAM to include tensor-product interaction terms between
clinically meaningful feature pairs like tumor size and boundary
irregularity. Candidate interactions included: smoothness × concavity,
radius × concave points, and fractal dimension × concavity.

Justification for these interactions are as follows:
1. smoothness_worst x concavity_worst: Low smoothness (coarse boundary) and high concavity is a malignancy marker. There are different risks associated with smooth but concave masses compared to rough but concave ones. 
2. radius_worst x concave.points_mean: Concave points measure sharp inward curves on the tumor boundary whereas radius measures size. Large lesions with many concave points are signals of malignancy.
3. fractal_dimension_se x concavity_worst: Fractal dimension relates to edge complexity and high fractal dimension with strong concavity could indicate irregular growth.  

These interactions were implemented using tensor-product smooths to
accommodate differing scales and nonlinear dependence. Model comparison
using AIC and likelihood ratio tests demonstrated a statistically
significant improvement in fit over the purely additive GAM. We fit several different models with different interaction terms, but the final model was selected for having the largest drop in AIC, in @tbl-aic, as well as a significant p-value from the Chi-Squared test. @tbl-chi shows that the final p-value for the model with interactions compared to the model without was 0.002 which is significant past the $\alpha = 0.01$ level. 

```{r, eval=TRUE, tbl.cap = "AIC Comparison of GAM without and with Interactions", label = "tbl-aic"}
# Building the formula with these interactions
formula_best <- diagnosis ~ 
  s(texture_se, bs = "cr", k = 10) +
  s(compactness_se, bs = "cr", k = 10) +
  s(fractal_dimension_se, bs = "cr", k = 10) +
  concave.points_mean + radius_se + radius_worst +
  s(texture_worst, bs = "cr", k = 10) +
  s(concavity_worst, bs = "cr", k = 10) + 
  compactness_mean  + concave.points_mean + radius_se + 
  smoothness_se + radius_worst + smoothness_worst + concave.points_worst + 
  symmetry_worst + 
  ti(concave.points_mean, radius_worst, bs=c("cr", "cr")) + 
  ti(concavity_worst, smoothness_worst, bs=c("cr", "cr")) +
  ti(concavity_worst, fractal_dimension_se, bs=c("cr", "cr")) 


# Building the best model
mod_best <- bam(formula_best,
               data = train_df,
               family = binomial(link = "logit"),
               method = "fREML",
               discrete = TRUE,      # huge speed boost
               select = TRUE)

kable(AIC(mod_all, mod_best), caption="AIC Comparison of GAM without and with Interactions") |>
  kable_styling(latex_options = c("hold_position", "scale_down"),
                font_size = 9,
                full_width = FALSE)
```

```{r, eval=TRUE, tbl.cap = "Chi-square with GAM without and with interactions", label = "tbl-chi"}
anova_out = anova(mod_all, mod_best, test="Chisq")
anova_table <- as.data.frame(anova_out)
kable(anova_table, caption = "Chi-square with GAM without and with interactions")|>
  kable_styling(latex_options = c("hold_position", "scale_down"),
                font_size = 9,
                full_width = FALSE)
```

We then assessed the model performance of this GAM with interactions on the test dataset.

```{r, eval=TRUE, results = 'asis', tbl.cap="Confusion Matrix for Model with Interactions", out.width="80%", label="tbl-conf-max-inter"}
df_test_pred_best <- test_df |>
  mutate(
    .pred_prob  = predict(mod_best, newdata = test_df, type = "response"),
    .pred_link  = predict(mod_best, newdata = test_df, type = "link"),
    .pred_class = as.integer(.pred_prob >= opt_threshold),
    .obs        = as.integer(!!rlang::sym("diagnosis"))
  )

cm2_best <- caret::confusionMatrix(
  factor(df_test_pred_best$.pred_class, levels = c(0,1)),
  factor(df_test_pred_best$.obs, levels = c(0,1)),
  positive = "1"
)

#cm2_best$overall["Accuracy"]
#cm2_best$byClass[c("Sensitivity", "Specificity")]

cm2_best <- table(Predicted = df_test_pred_best$.pred_class, 
             Actual = df_test_pred_best$.obs)
cm_df2_best <- as.data.frame.matrix(cm2_best)
cm_labeled2_best <- data.frame(
  Outcome = c("Predicted Benign", "Predicted Malignant"),
  Benign  = cm2_best[, "0"],
  Malignant = cm2_best[, "1"]
)
kable(cm_labeled2_best, caption = "Confusion Matrix for Model with Interactions", align = "c")|>
  kable_styling(latex_options = c("hold_position", "scale_down"),
                font_size = 9,
                full_width = FALSE)

roc_obj_best <- roc(df_test_pred_best$.obs, 
               df_test_pred_best$.pred_prob, quiet = TRUE)
auc_val_best <- as.numeric(pROC::auc(roc_obj))
#print(auc_val_best)
```
We observe that the Confusion matrix in @tbl-conf-max-inter is identical to that with the GAM without interactions in @tbl-conf-mat-no-inter, but this is likely due to the small sample size. Additionally, the accuracy, sensitivity, and specificity remain the same but this can likely also be attributed to the small sample size and the strong performance of the original model without interactions.

```{r, eval = FALSE}
summary(mod_best)
```
Based on the output to our GAM model with interactions, the linear predictors that appear to have statistically significant relationships with tumor malignancy are mean concave points, radius SE, radius worst, and mean compactness, with trends consistent to the model without interactions. Nonlinear predictors that are statistically significant include texture SE and worst SE, consistent to the model without interactions. The tested interaction terms were not statistically significant; however, this model with interactions is still chosen due to higher predictive performance.  

## Final Model Selection

The GAM with selected nonlinear terms and domain-motivated interactions
was chosen as the final model, as it achieved strong predictive
performance (high AUC and balanced sensitivity/specificity), captured
clinically plausible nonlinear and interaction effects, remained
interpretable through smooth and interaction visualizations, and aligned
with the project’s emphasis on transparency and trust in medical
decision making. The final sensitivity was 0.981 which is very high since 98.1% of malignancy cases were correctly identified in the model, so only 1.9% of cases are false negatives which we want to minimize. 

## Downstream Uses of the Model

Our final predictive model can serve several downstream purposes.
Clinicians may use a model derived malignancy probability as a
preliminary triage tool to help determine urgency of follow up imaging
or need for biopsy. Researchers may use our inferred feature importance
patterns to better understand structural characteristics of malignant
tissue, signaling which measurements warrant closer attention in future
imaging protocols. In healthcare settings, such tools can also guide
resource allocation by highlighting high risk cases sooner, especially
when imaging workloads are high (with over 42 million mammograms
performed annually across the US and UK) [@nature].

By ensuring that our model is interpretable and clearly justified, it
can be used ethically and effectively in supporting clinical workflows,
improving early detection, and helping reduce false positives which have
been long recognized as a challenge in mammographic screening.

# **Results**

## Key Findings

Since both our final GAM models have essentially identical predictive results, we have two potential models for clinicians to choose between. If they want a simpler model with fewer features, the GAM without interactions would be a great predictive model to use. On the other hand if they want to prioritize more complex models to explain ways that certain features interact, they should use the GAM with interactions.

Although the LASSO logistic model wasn't selected as our final model for predictions, the features it identified as key features are very helpful for understanding which features of a mass are most indicative of the mass' probability of being malignant. Identifying these features allows clinicians to grasp which aspects of the mass they should focus on when they receive the data by seeing if any of these selected 13 features have values outside of a normal range.

## Limitations

One limitation is that the data was acquired in 1995. While our work provides proof of concept modeling, future work can expand upon these findings by incorporating modern imaging technology (3D mammography, MRI, MBI) that are capable of detecting additional tumor features. Additionally, we are working with a fairly small sample size of observations (569 total). We would expect to see better predictive accuracy and improvement in sensitivity when adding interactions if we had a larger test dataset.  

Another limitation is that the features are computed from summary statistics for each tumor (mean, SE, "worst"). Although having these values provide general trends, they may fail to capture specific nuances to tumor characteristics compared to using raw imaging data. Furthermore, the dataset currently examines malignant tumor masses that are in stage 1 or stage 2, which may limit generalizability; future work can assess whether similar findings hold for more advanced stages of breast cancer.

Lastly, there are limitations to the modeling assumptions for lasso logistic regression and GAMs. Important nonlinear or interaction effects may be overlooked since lasso regression relies on linear assumptions, and although GAMs may capture some non-linear patterns, due to their additive nature, they may still fail to capture complex, higher-order interactions between tumor features.

# **Conclusion**

Further analysis in the cancer field is pivotal as healthcare advancements are vast and rapidy growinf to fulfill the needs of patients. With that, we have prioritized recommendations on how to best optimize future research on Breast Cancer Diagnosis. We suggest investigating the interaction effects between stages of cancer and existing variables. We affirm that this insight would provide important information to healthcare professionals when it comes to assessing urgency in diagnosis. According to the CDC, most breast cancer diagnosis are caught in the early stages, but when caught in a later stage, the cancer has likely spread to other organs which would coincide with the results we gathered from our model (ie, as radius increases so does the probability of cancer) [@cdc_stats]. 

Another suggestion for future work would be to include age, race/ethnicity and socioeconomic class in the data. Not only would this allow us to see how we can ensure proper representation, but also, according to the American Cancer Society, these three factors actually play a role in what stage of cancer (and thus the probability the tumor is malignant or begin) the diagnosis is received [@acs_facts]. 

Our findings have crafted the framework for future researchers to continuously test our model with recent data, and we encourage others to expand on our results with suggestions above or beyond.
