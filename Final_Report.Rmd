---
title: "Predicting Breast Tumor Malignancy Using Generalized Additive Models"
author: "Laura Cai, Ava Exelbirt, Abby Li, Grady Purcell, Ella Tillinghast"
date: "`r Sys.Date()`"
editor_options:
  markdown:
    wrap: 72
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,        # hide code by default
  message = FALSE,     # hide messages
  warning = FALSE,     # hide warnings
  fig.align = "center"
)

```


# **Introduction**

## Project Goals

The primary goal of our project is to build an interpretable machine
learning model that can assess whether a breast mass is benign or
malignant using measurable characteristics such as radius, area,
smoothness, and concavity extracted from digitized medical images. Our
analysis has two complementary aims: prediction, where we train a
classifier to estimate the probability that a new mass is malignant, and
inference, where we identify and interpret the features that have the
greatest influence on these probability assignments. Because our target
audience includes clinicians, researchers, and individuals without a
technical background, we emphasize transparency and interpretability,
focusing not only on whether the model predicts accurately, but also on
why it arrives at its predictions.

The project also addresses several relevant challenges, including
potential noise in imaging, derived features, limitations associated
with an older dataset, and the need for models that doctors can trust
when making time-sensitive decisions. Trust and interpretability are
central concerns in medical applications, and our modeling pipeline is
intentionally designed to balance performance with transparency,
ensuring that the results are clinically meaningful and accessible.

## Importance and Relevance

Breast cancer is one of the most pressing health challenges worldwide
and remains the second most common cancer among women, accounting for
roughly 30% of all new female cancer cases each year, and the second
leading cause of cancer related deaths among women (American Cancer
Society, 2024). As incidence rates continue to rise by approximately 1%
annually, improving tools for early differentiation between benign and
malignant tumors is critical for timely intervention. Early detection of
breast cancer has been shown to boost survival rates by up to 20%,
further underscoring the clinical relevance of high quality diagnostic
support tools (ClinicalKey, 2025).

Traditional diagnostic techniques like mammography, ultrasound, MRI, and
biopsy are essential but can also be expensive, time consuming, and
prone to human error, particularly false positives and false negatives.
Recent research demonstrates that machine learning (ML) can meaningfully
strengthen diagnostic processes. For instance, ML models have shown
higher accuracy than clinicians in predicting several cancers, including
breast, brain, and lung cancer (PMC10312208). A 2020 DeepMind based
system outperformed human specialists in breast cancer detection
(Nature, 2020), and other ML systems have reached 97% accuracy in
identifying common types of lung cancer (PMC10312208). These findings
highlight the potential for ML driven tools to enhance clinical
workflows and reduce diagnostic uncertainty.

Even within the specific Wisconsin Breast Cancer dataset used here,
prior research has achieved strong performance: Support Vector Machines
have reached 87–89% accuracy in distinguishing malignant from benign
masses, and deep learning models such as CNNs have successfully
extracted subtle features (e.g., microcalcifications or architectural
distortions) that radiologists may overlook (ClinicalKey, 2025).

Although the dataset we analyze is more than two decades old, our
project serves as a proof of concept illustrating how interpretable ML
methods can support clinical reasoning. Because modern imaging systems
continue to quantify many of the same structural features used in this
dataset, our modeling framework remains relevant, scalable, and
adaptable. By identifying the features most strongly associated with
malignancy, we provide insights that can inform research, guide
clinicians’ early decision making, and support the development of future
diagnostic tools designed to complement, not replace, medical expertise.

## Overview of Proposed Methodology

Since we are building our model to assist doctors with diagnosing cancer patients, our model needs to be interpretable so doctors are more confident in their diagnosis and can explain the factors which led to the final diagnosis. To do this, we will first use a LASSO logistic regression model to shrink the features so that only relevant features are included in the final model. Shrinkage is important in this setting because it is likely that there is multicorrelation between many of the features so we don't want our final model to include all of the features. Once the LASSO logistic model provides the relevant features, we will build a Generalized Additive Model (GAM) since our EDA has indicated nonlinearity between the predictors and state of the tumor, so we want our final model to have a balance between flexibility and interpretability. After selecting which features should be included as non-linear splines in the model, we will then include interactions based on exploratory analysis, domain knowledge, and model performance. 

# **Data**

The dataset used in this analysis comes from the Diagnostic Wisconsin
Breast Cancer Database (WBCD), originally created from digitized images
of fine needle aspiration (FNA) biopsies taken from breast masses. The
version used here was downloaded from Kaggle, but the source dataset was
donated on October 31, 1995. Although imaging modalities have improved
significantly since 1995, modern systems continue to quantify similar
structural and morphological features, meaning that this dataset remains
an informative foundation for building a proof-of-concept modeling
framework that can later be extended to more advanced imaging
technologies.

Each observation corresponds to a single breast mass and contains 30
continuous predictors, derived from 10 underlying cell nucleus features:
radius, texture, perimeter, area, smoothness, compactness, concavity,
concave points, symmetry, fractal dimension. Each of these ten features
was recorded in three forms:

1.  Mean – the average value across the tumor sample
2.  Standard Error (SE) – the variability of that feature
3.  Worst (maximal) – the mean of the highest three values

The response variable is the diagnosis: malignant (M) or benign (B).

There are 569 observations, including 357 benign (62.7%) and 212
malignant (37.2%) cases. Although benign tumors are more common in the dataset, 
the dataset is only moderately imbalanced, so rebalancing procedures such as
SMOTE or class weighting are not necessary for this analysis.

The data contains no missing values, and all predictors were
standardized with four significant digits in the version we downloaded.
Because the features are continuous, structured, and well behaved, this
dataset is well suited for multiple modeling approaches including
logistic regression, LASSO, KNN, random forests, and decision trees.

Finally, this dataset provides a solid platform for exploratory data
analysis (EDA) and model interpretation because each predictor has a
clear biological meaning and can be reasoned about in clinical terms
(e.g., radius relates to tumor size, concavity relates to irregularity).

The 13 predictors selected from the LASSO logistic model can be grouped into three categories: those which measure mean, standard error, and worst. The mean variables include compactness and concave points. The standard error features are radius, texture, smoothness, compactness and fractal dimension. Lastly, the worst variables are radius, texure, smoothness, concavity, concave points and symmetry.

# **Modeling Methodology**

Our methodology consists of five main stages: exploratory data
analysis, threshold optimization and evaluation, preliminary model
comparison, regularized model selection, and nonlinear modeling with
generalized additive models (GAMs).

## Exploratory Data Analysis (EDA)

```{r, eval=TRUE}
# Loading the packages
library(tidyverse)
library(ggplot2)
library(dplyr)
library(class)
library(caret)
library(pROC)
library(glmnet)
library(GGally)
library(mgcv)
library(tidyr)
library(knitr)
library(kableExtra)
```

We began by conducting a comprehensive exploratory data analysis to
understand the distribution of each of the 30 predictors and to identify
any necessary transformations or outlier adjustments. Visualizations
included histograms, boxplots, and scatterplots across both classes
(benign vs. malignant).

Although certain variables showed skewed distributions, which is
expected in biological measurements, none raised concerns severe enough
to justify transformation. Outliers were examined but ultimately retained, 
as they reflect real biological variation rather than measurement error. 
No missing data were present, so no imputation or deletion procedures were 
required. The EDA thus served primarily to guide expectations about predictor 
behavior and to inform our choice of appropriate models.

```{r, results = 'asis', fig.cap="Figure 1: EDA for Interaction Terms", out.width="80%"}
library(patchwork)
df <- read.csv("Data/data.csv")

# smoothness_worst vs concavity_worst
p1 <- ggplot(df, aes(smoothness_worst, concavity_worst, color = diagnosis)) +
  geom_point() +
  geom_smooth(method = "gam") + 
  ggtitle("Effect of Smoothness (Worst) and Concavity (Worst) on Diagnosis")

# radius_worst vs concave.points_mean
p2 <- ggplot(df, aes(radius_worst, concave.points_mean, color = diagnosis)) +
  geom_point() +
  geom_smooth(method = "gam") + 
  ggtitle("Effect of Radius (Worst) and Concave Points (Worst) on Diagnosis")

# fractal_dimension_se vs concavity_worst
p3 <- ggplot(df, aes(fractal_dimension_se, concavity_worst, color = diagnosis))+
  geom_point() +
  geom_smooth(method = "gam") + 
  ggtitle("Effect of Fractal Dimension (SE) and Concavity (Worst) on Diagnosis")


(p1 | p2 | p3) + plot_layout(guides = "collect")
```

We also explored bivariate EDA to examine potential interaction effects between predictors. In Figure 1, there appears to be interaction effects between each of the listed pairs of predictors since the relationships between each pair differs with the benign and malignant tumors. 

## Cost Analysis and Threshold Selection 

We researched the cost of false positives and negatives to compute
a cost based loss function to determine the optimal classification
threshold. This threshold will reflect the asymmetric cost of false
negatives (missing a malignant tumor) versus false positives
(unnecessary further testing), and will guide our final decision rule
applied to our model. 

As a result, our threshold selection prioritizes maximizing sensitivity to minimize false negatives. False-negative results can lead to delayed detection and treatment, resulting in greater long-term costs, morbidity, and mortality. One study estimated the national cost-savings in the U.S. from early diagnosis of cancers to be $26 billion per year due to reduced treatment intensity and improved outcomes (Brill, 2020). 

Prior research also shows that the accuracy of mammography screening increases with patient age, with sensitivity ranging from 76% to 86% and specificity ranging from 87% to 99% across age groups. (Newton, 2025). This gap in threshold shows why maximizing sensitivity matters: since detecting malignant tumors is the harder task, optimizing sensitivity is essential to improving screening effectiveness. Thus, our decision to choose a lower threshold is consistent with addressing current screening practices and reducing future costs associated with delayed diagnoses. 

## Preliminary Model Fitting

We next fit several classification models to evaluate baseline
predictive performance and to assess the trade-offs between accuracy,
interpretability, computational complexity, and clinical
trustworthiness. The models included: K-Nearest Neighbors (KNN), Random
Forest, LASSO, Standard Logistic Regression, and Decision Trees. Each
model was then evaluated using standard metrics including accuracy,
sensitivity, specificity, and confusion matrices. Cross validation was
used where appropriate to reduce sampling variability.

Although the random forest achieved strong predictive accuracy, it
lacked interpretability which should be prioritized since this model will be used by doctors to diagnose whether a breast cancer mass is benign or malignant at an early stage. 
KNN performed reasonably but does not provide coefficients, feature weights, or insight into mechanism. Standard logistic regression was interpretable but tended to overfit
without regularization due to the high number of correlated predictors.

## Model Selection and Rationale

Based on accuracy, interpretability, stability, and feature selection
capability, we selected the LASSO logistic regression model as our
primary predictive and inferential tool. LASSO automatically performs
variable selection by shrinking less important coefficients to zero,
providing a concise, clinically meaningful set of predictors and
potential interactions.

After selecting these top predictors, we further fit a GAM informed by
the LASSO selected features. We selected a GAM model for interpretability and flexibility in including nonlinear terms. GAM models are more interpretable than many other regression techniques since they enable visualization between features and the predicted malignancy probability with interpretability being essential for clinical decisions. Additionally, the nonlinearlity of GAMs is advantageous as tumor characteristics rarely relate to malignancy in linear ways, so GAMs can model predictors using a smooth function like splines to learn curves without assuming linearity which improves predictive performance. 

## Regularized Logistic Regression and Feature Selection

We first fit a LASSO logistic model to perform variable selection and establish a strong, interpretable baseline classifier. The diagnosis variable was
encoded as 1 for malignant and 0 for benign. Identifier columns were
removed, and the remaining predictors were standardized. The data were
randomly split into training (80%) and testing (20%) sets. A LASSO
logistic regression model was fit using 5-fold cross-validation to
select the optimal penalty parameter with `cv.glmnet`. This approach
addresses multicollinearity among predictors and prevents overfitting by
shrinking less informative coefficients to zero. The final LASSO logistic model
achieved approximately 99% accuracy on the held-out test set, with
perfect sensitivity and strong specificity. 

### Setting the Optimal Threshold
From the LASSO logistic regression, we determined the optimal threshold to predict whether a mass is benign or malignant based on the assigned probability. From the cost analysis and domain knowledge, we knew that the optimal threshold would maximize sensitivity. Using the `proc` library, we found that the optimal threshold to achieve perfect sensitivity is 0.351 using Youden's J statistic. Youden's method is a way to choose the "best" classification threshold by maximizing the combined performnace of sensitiivty and specificity. The optimal threshold is one that maximizes J = Sensitivity + Specificity - 1. For this diagnostic classification problem, Youden's J statistic chooses the threshold that provides a balance weight to correctly identifying malignant and benign cases. Since cancer diagnosis requires higher sensiviity to avoid missing malignant cases, we want the threshold to be less than 0.5 so the false negative rate is decreased, so 0.351 seems a reasonable value. Other benefits for using Youden's statistics is that it is not distorted by how common cancer is in the dataset and it is transparet for clinicians. 

Although perfect sensitivity often comes at the cost of poor specificity and many false positives, we also observe a high specificity of 0.98 with perfect sensitivity. Hence, we are less concerned with the model classifying every observation as benign at this threshold given that the specificity is also very high. For the rest of the paper, 0.351 will be the threshold used for classifying a tumor. 

```{r}
df <- read.csv("Data/data.csv")
df <- df |> select(-X)
df <- data.frame(df)

# Adding a categorical variable for diagnosis
df$diagnosis <- ifelse(df$diagnosis == "M", 1, 0)
df <- df[ , !names(df) %in% c("id", "X", "Unnamed..32") ] # remove id columns

# Preparing the data for the train/test split
X <- model.matrix( ~ ., data = df)[, -1]
y <- df$diagnosis

# Train/test split
set.seed(123) 
n <- nrow(X)
train_idx <- sample(seq_len(n), size = 0.8 * n)

X_train <- X[train_idx, ][, -1]
X_test  <- X[-train_idx, ][, -1]
y_train <- y[train_idx]
y_test  <- y[-train_idx]

# Building the LASSO model
cvfit <- cv.glmnet(
  x = X_train,
  y = y_train,
  family = "binomial",
  alpha = 1,        # LASSO
  nfolds = 5        # 5-fold CV
)

# Predicted probabilities (logistic) at lambda.min
prob_test <- predict(cvfit, newx = X_test, 
                     s = "lambda.min", 
                     type = "response")
prob_test <- as.numeric(prob_test)

roc_obj <- roc(response = y_test, predictor = prob_test, positive = "1")

# Extracting the optimal threshold
coords <- coords(roc_obj,
       x = 1,
       input = "sensitivity",
       ret = c("threshold", "sensitivity", "specificity"))
opt_threshold <- coords$threshold

# Predicted class based on the optimal threshold
pred_class <- ifelse(prob_test >= opt_threshold, 1, 0)
```

```{r, eval=TRUE, results = 'asis', fig.cap="Figure 2: Confusion Matrix", out.width="80%"}
# Building the Confusion Matrix
cm <- table(Predicted = pred_class, Actual = y_test)
cm_df <- as.data.frame.matrix(cm)
cm_labeled <- data.frame(
  Outcome = c("Predicted Benign", "Predicted Malignant"),
  Benign  = cm[, "0"],
  Malignant = cm[, "1"]
)
kable(cm_labeled, caption = "Confusion Matrix", align = "c")
```

### Model Diagnostics 
The Confusion Matrix above uses the optimal threshold and achieves perfect sensitivity, a specificity of 0.98, and a 99% accuracy. This indicates strong overall model performance and although there are possible concerns about model overfitting, we are confident in our results given that we used 5-fold CV to train the data and then tested the data on the hold-out set.  
```{r}
accuracy <- mean(pred_class == y_test)
sens <- sum(pred_class == 1 & y_test == 1) / sum(y_test == 1)
spec <- sum(pred_class == 0 & y_test == 0) / sum(y_test == 0)
#c(accuracy = accuracy, sensitivity = sens, specificity = spec)
```

Model performance was further evaluated using an ROC curve and a measure of AUC, providing a threshold-independent measure of discrimination. The AUC for the ROC curve is 0.9994 which is fairly high, giving us confidence in our model. 

The last step after building the LASSO logistic model was extracting the features which had non-zero coefficients for predicting the probability of a tumor being benign or malignant. Below is a list of the 13 features (out of 30) which the LASSO model indicated were important for predicting the probabilities ordered in descending order of magnitude. 
```{r, eval=TRUE}
# Extracting the coefficient features
coef_min <- coef(cvfit, s = "lambda.min")
coef_df <- as.data.frame(as.matrix(coef_min))
coef_df$feature <- rownames(coef_df)
names(coef_df)[1] <- "coefficient"

# Filtering for coefficients which aren't 0
feature_list <- coef_df |>
  filter(feature != "(Intercept)") |>
  filter(coefficient != 0) |>
  arrange(desc(abs(coefficient))) |>
  pull(feature)

# Grouping by the values they end in
group_se    <- feature_list[grepl("_se$", feature_list)]
group_mean  <- feature_list[grepl("_mean$", feature_list)]
group_worst <- feature_list[grepl("_worst$", feature_list)]

# Determine longest group
max_len <- max(length(group_se), length(group_mean), length(group_worst))

# Pad groups to same length
group_se_padded    <- c(group_se,    rep("", max_len - length(group_se)))
group_mean_padded  <- c(group_mean,  rep("", max_len - length(group_mean)))
group_worst_padded <- c(group_worst, rep("", max_len - length(group_worst)))

# Build final data frame
df_grouped <- data.frame(
  Mean  = group_mean_padded,
  SE    = group_se_padded,
  Worst = group_worst_padded
)

kable(
  df_grouped,
  caption = "Selected Features",
  align = "c"
)
```

In summary, the LASSO logistic model yielded a parsimonious subset of clinically
interpretable predictors, which served two purposes:

1.  Establishing a transparent, high-performing baseline classifier.
2.  Guiding subsequent nonlinear modeling by identifying variables most
    strongly associated with malignancy.

## Nonlinearity Assessment and Motivation for GAMs

While LASSO logistic regression provides interpretability through
coefficients, it assumes linear relationships between predictors and the
log-odds of malignancy. To assess whether this assumption was
appropriate, we conducted a systematic investigation of potential
nonlinearity.

For each predictor selected by the LASSO model, we binned the predictor
into quantiles, computed observed malignancy proportions within each
bin, and examined both probability-scale and log-odds-scale plots.
Several predictors, most notably texture_se, compactness_se,
fractal_dimension_se, texture_worst and concavity_worst, exhibited clear nonlinear patterns,
suggesting that a purely linear model may be misspecified.

## Generalized Additive Models (GAMs)

To flexibly model these nonlinear effects while preserving
interpretability, we employed Generalized Additive Models (GAMs) with a
binomial logit link. GAMs model the log-odds of malignancy as a sum of
smooth functions of predictors, allowing nonlinear relationships to be
captured without requiring explicit interaction terms.

We compared different spline bases (thin-plate regression splines,
cubic regression splines, and thin-plate shrinkage splines) for individual predictors using AIC, REML
scores, and visual inspection. Restricted Maximum Likelihood (REML) was used to compare the splines instead of Maximum Likelihood because REML provides less biased estimates for variance components. Although no spline performed definitively better across all metrics, cubic splines generally had lower AIC and REML across variables. Additionally, cubic splines had a lower range of plausible predicted probabilities and less wiggly than the other two splines. Thus, we chose to use cubic regression splines since consistently
provided better stability and more realistic probability behavior, and
were therefore selected.

After determining the appropriate spline to be cubic splines, we selected which features show strong violation of nonlinearity so we can include them in splines. All other variables which don't show strong evidence of nonlinearity will be included as main effects or interaction terms. To do this, we first binned each of the predictors and calculated the appropriate log-odds of malignancy within the training data. We then visualized the logit plot for each predictor to determine where we could see strong violations of nonlinearity. 

```{r}
# Getting the training dataframe
X2 <- model.matrix( ~ ., data = df)[, -1]

# Getting the dataframes
train_df <- as.data.frame(X2[train_idx, ])
test_df <- as.data.frame(X2[-train_idx, ])
```

```{r, eval = TRUE, results = 'asis', fig.cap="Figure 3: Log-Odds Plots", out.width="80%"}
nbins <- 20
yvar <- "diagnosis"       # your binary outcome

# Function that bins & plots one variable
make_binned_plots <- function(df, xvar, yvar, nbins = 20) {
  
  # 1) Compute bins
  binned <- df %>%
    mutate(bin = ntile(.data[[xvar]], nbins)) %>%
    group_by(bin) %>%
    summarise(
      x_mid = mean(.data[[xvar]], na.rm = TRUE),
      n     = n(),
      prop  = mean(.data[[yvar]], na.rm = TRUE),
      .groups = "drop"
    ) %>%
    mutate(
      se    = sqrt(prop * (1 - prop) / pmax(n, 1)),
      lower = pmax(0, prop - 1.96 * se),
      upper = pmin(1, prop + 1.96 * se),
      eps   = 1e-3,
      logit = log((prop + eps) / (1 - prop + eps))
    )
  
  # 2) Proportion plot
  p_prop <- ggplot(binned, aes(x = x_mid, y = prop)) +
    geom_point(size = 2) +
    geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.01) +
    labs(
      title = paste("Binned proportion for", xvar),
      x = xvar,
      y = "Observed proportion"
    ) +
    theme_minimal()
  
  # 3) Logit plot
  p_logit <- ggplot(binned, aes(x = x_mid, y = logit)) +
    geom_point(size = 2) +
    geom_smooth(method = "loess", se = TRUE) +
    labs(
      title = paste("Binned log-odds for", xvar),
      x = xvar,
      y = "Estimated log-odds"
    ) +
    theme_minimal()
  
  return(list(binned = binned, prop_plot = p_prop, logit_plot = p_logit))
}

# ---- Loop over all predictors ----
results <- map(feature_list, ~ make_binned_plots(train_df, .x, yvar))
names(results) <- feature_list

## Showing all of the plots
p1 <- results[["smoothness_se"]]$logit_plot
p2 <- results[["concave.points_mean"]]$logit_plot
p3 <- results[["radius_se"]]$logit_plot
p4 <- results[["radius_worst"]]$logit_plot
p5 <- results[["texture_worst"]]$logit_plot
p6 <- results[["smoothness_worst"]]$logit_plot
p7 <- results[["concavity_worst"]]$logit_plot
p8 <- results[["concave.points_worst"]]$logit_plot
p9 <- results[["symmetry_worst"]]$logit_plot
p10 <- results[["compactness_mean"]]$logit_plot
p11 <- results[["texture_se"]]$logit_plot
p12 <- results[["compactness_se"]]$logit_plot
p13 <- results[["fractal_dimension_se"]]$logit_plot

# Arranging the plots
plots <- list(
  p1, p2, p3, p4, p5, p6,
  p7, p8, p9, p10, p11, p12, p13
)

wrap_plots(plots, ncol = 4) & 
  theme(legend.position = "bottom")

```

In Figure 3, we observe that many of the variables to violate nonlinearity. Thus, we also want to quantitatively determine which features should be included as splines by comparing a GLM model to predict malignancy probability with a GAM using only a cubic spline with the selected feature. Then, we compared the change in AIC between the GLM and GAM as well as the p-value for the Chi-Squared test where the cutoff was set at $\alpha = 0.01$. Features which had a drop in AIC with a magnitude of at least 4 and a significant p-value were identified as features with strong evidence of non-linearity. The delta AIC value of 4 was selected using the convention that a drop in AIC greater than 4 is evidence that the GAM meaningfully improves the model fit. 

```{r, eval=TRUE}
results <- map_dfr(feature_list, function(var) {
  
  # build formulas
  form_glm <- as.formula(paste("diagnosis ~", var))
  form_gam <- as.formula(paste("diagnosis ~ s(", var, ", k=10)", sep = ""))
  
  # fit models
  mod_glm <- glm(form_glm, data=train_df, family = binomial("logit"))
  mod_gam <- gam(form_gam, data=train_df, bs="cr",
                 family = binomial("logit"), method="REML")
  
  # AIC comparison
  aic_vals <- AIC(mod_glm, mod_gam)
  
  # Likelihood ratio test (nested model test)
  an <- anova(mod_glm, mod_gam, test="Chisq")
  pval <- an$`Pr(>Chi)`[2]   # GAM vs GLM

  
  tibble(
    variable = var,
    glm_AIC  = aic_vals$AIC[1],
    gam_AIC  = aic_vals$AIC[2],
    delta_AIC = aic_vals$AIC[1] - aic_vals$AIC[2],
    LRT_pvalue = an$`Pr(>Chi)`[2],  # the p-value comparing GLM vs GAM
    significant_0.01 = ifelse(!is.na(pval) & pval < 0.01, "YES", "NO")

  )
})

results |> 
  filter(abs(delta_AIC) > 4, significant_0.01 == "YES") |>
  select(variable, delta_AIC) |>
  kable()

```

From the results above, the following features were included in the GAM as cubic splines: texture_se, compactness_se, fractal_dimension_se, texture_worst, and concavity_worst. 

From here, the initial GAM model was built using cubic splines for 5 nonlinear features and including the other 8 features as main effects. 
```{r}
formula_all <- diagnosis ~ 
  s(texture_se, bs = "cr", k = 10) +
  s(compactness_se, bs = "cr", k = 10) +
  s(fractal_dimension_se, bs = "cr", k = 10) +
  concave.points_mean + radius_se + radius_worst +
  s(texture_worst, bs = "cr", k = 10) +
  s(concavity_worst, bs = "cr", k = 10) + 
  compactness_mean  + concave.points_mean + radius_se + 
  smoothness_se + radius_worst + smoothness_worst + concave.points_worst + 
  symmetry_worst

mod_all <- bam(formula_all,
               data = train_df,
               family = binomial(link = "logit"),
               method = "fREML",
               discrete = TRUE,      # huge speed boost
               select = TRUE)
```



Formal model comparisons between generalized linear models and GAMs
using AIC differences and likelihood ratio tests, confirmed strong
evidence of nonlinearity for several predictors. Based on these results,
smooth terms were retained for smoothness_se, texture_worst, and
concavity_worst. Other predictors entered the model linearly.

## Final Additive GAM Specification

The resulting additive GAM incorporated nonlinear smooth terms for
predictors with strong evidence of nonlinearity, along with linear
effects for remaining LASSO-selected features. Model diagnostics,
including residual plots, k-index checks, and concurvity analysis,
indicated adequate fit and no severe violations of modeling assumptions.
Model performance was evaluated using confusion matrices, ROC curves,
and AUC. The GAM demonstrated strong discriminatory ability and improved
calibration relative to the purely linear model, while remaining
interpretable through smooth effect plots.

## Incorporating Interaction Effects

Although GAMs capture nonlinear marginal effects, additive models may
miss important interactions. Guided by domain knowledge and prior
findings on the Wisconsin Breast Cancer Dataset, we extended the
additive GAM to include tensor-product interaction terms between
clinically meaningful feature pairs like tumor size and boundary
irregularity. Candidate interactions included: concave points × radius,
concavity × radius, smoothness × concavity, and texture × concavity.

These interactions were implemented using tensor-product smooths to
accommodate differing scales and nonlinear dependence. Model comparison
using AIC and likelihood ratio tests demonstrated a statistically
significant improvement in fit over the purely additive GAM.

## Final Model Selection

The GAM with selected nonlinear terms and domain-motivated interactions
was chosen as the final model, as it achieved strong predictive
performance (high AUC and balanced sensitivity/specificity), captured
clinically plausible nonlinear and interaction effects, remained
interpretable through smooth and interaction visualizations, and aligned
with the project’s emphasis on transparency and trust in medical
decision making.

## Downstream Uses of the Model

Our final predictive model can serve several downstream purposes.
Clinicians may use a model derived malignancy probability as a
preliminary triage tool to help determine urgency of follow up imaging
or need for biopsy. Researchers may use our inferred feature importance
patterns to better understand structural characteristics of malignant
tissue, signaling which measurements warrant closer attention in future
imaging protocols. In healthcare settings, such tools can also guide
resource allocation by highlighting high risk cases sooner, especially
when imaging workloads are high (with over 42 million mammograms
performed annually across the US and UK, Nature 2020).

By ensuring that our model is interpretable and clearly justified, it
can be used ethically and effectively in supporting clinical workflows,
improving early detection, and helping reduce false positives which have
been long recognized as a challenge in mammographic screening.

# **Results**

## Limitations

One limitation is that the data is from 1995. While our work provides proof of concept modeling, future work can expand upon these findings by utilizing modern imaging technology (3D mammography, MRI, MBI) that can can detect additional features. Another limitation is that the predictors are based on computed summary statistics for each tumor (mean, SE, "worst"). Although having these values provide general trends, they may fail to capture specific nuances to tumor characteristics compared to using raw imaging data. An additional limitation to the models selected, lasso logistic regression and GAMs, is that important nonlinear or interaction effects may be overlooked, since regression relies on linear assumption and GAMs may fail to capture complex, higher-order interactions between tumor features. 

# **Conclusion**

## Future Analysis

# **Reference**
Github: https://github.com/gradypurcell/STA-325-Final-Project.git 