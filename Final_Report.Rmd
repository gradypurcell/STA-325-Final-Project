---
title: "Predicting Breast Tumor Malignancy Using Generalized Additive Models"
author: "Laura Cai, Ava Exelbirt, Abby Li, Grady Purcell, Ella Tillinghast"
date: "`r Sys.Date()`"
editor_options:
  markdown:
    wrap: 72
output: pdf_document
geometry: margin=0.5in
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,        # hide code by default
  message = FALSE,     # hide messages
  warning = FALSE,     # hide warnings
  fig.align = "center"
)

```


# **Introduction**

## Project Goals

The primary goal of our project is to build an interpretable machine
learning model that can assess whether a breast mass is benign or
malignant using measurable characteristics such as radius, area,
smoothness, and concavity extracted from digitized medical images. Our
analysis has two complementary aims: prediction, where we train a
classifier to estimate the probability that a new mass is malignant, and
inference, where we identify and interpret the features that have the
greatest influence on these probability assignments. Because our target
audience includes clinicians, researchers, and individuals without a
technical background, we emphasize transparency and interpretability,
focusing not only on whether the model predicts accurately, but also on
why it arrives at its predictions.

The project also addresses several relevant challenges, including
potential noise in imaging, derived features, limitations associated
with an older dataset, and the need for models that doctors can trust
when making time-sensitive decisions. Trust and interpretability are
central concerns in medical applications, and our modeling pipeline is
intentionally designed to balance performance with transparency,
ensuring that the results are clinically meaningful and accessible.

## Importance and Relevance

Breast cancer is one of the most pressing health challenges worldwide
and remains the second most common cancer among women, accounting for
roughly 30% of all new female cancer cases each year, and the second
leading cause of cancer related deaths among women (American Cancer
Society, 2024). As incidence rates continue to rise by approximately 1%
annually, improving tools for early differentiation between benign and
malignant tumors is critical for timely intervention. Early detection of
breast cancer has been shown to boost survival rates by up to 20%,
further underscoring the clinical relevance of high quality diagnostic
support tools (ClinicalKey, 2025).

Traditional diagnostic techniques like mammography, ultrasound, MRI, and
biopsy are essential but can also be expensive, time consuming, and
prone to human error, particularly false positives and false negatives.
Recent research demonstrates that machine learning (ML) can meaningfully
strengthen diagnostic processes. For instance, ML models have shown
higher accuracy than clinicians in predicting several cancers, including
breast, brain, and lung cancer (PMC10312208). A 2020 DeepMind based
system outperformed human specialists in breast cancer detection
(Nature, 2020), and other ML systems have reached 97% accuracy in
identifying common types of lung cancer (PMC10312208). These findings
highlight the potential for ML driven tools to enhance clinical
workflows and reduce diagnostic uncertainty.

Even within the specific Wisconsin Breast Cancer dataset used here,
prior research has achieved strong performance: Support Vector Machines
have reached 87–89% accuracy in distinguishing malignant from benign
masses, and deep learning models such as CNNs have successfully
extracted subtle features (e.g., microcalcifications or architectural
distortions) that radiologists may overlook (ClinicalKey, 2025).

Although the dataset we analyze is more than two decades old, our
project serves as a proof of concept illustrating how interpretable ML
methods can support clinical reasoning. Because modern imaging systems
continue to quantify many of the same structural features used in this
dataset, our modeling framework remains relevant, scalable, and
adaptable. By identifying the features most strongly associated with
malignancy, we provide insights that can inform research, guide
clinicians’ early decision making, and support the development of future
diagnostic tools designed to complement, not replace, medical expertise.

## Overview of Proposed Methodology

Since we are building our model to assist doctors with diagnosing cancer patients, our model needs to be interpretable so doctors are more confident in their diagnosis and can explain the factors which led to the final diagnosis. To do this, we will first use a LASSO logistic regression model to shrink the features so that only relevant features are included in the final model. Shrinkage is important in this setting because it is likely that there is multicorrelation between many of the features so we don't want our final model to include all of the features. Once the LASSO logistic model provides the relevant features, we will build a Generalized Additive Model (GAM) since our EDA has indicated nonlinearity between the predictors and state of the tumor, so we want our final model to have a balance between flexibility and interpretability. After selecting which features should be included as non-linear splines in the model, we will then include interactions based on exploratory analysis, domain knowledge, and model performance. 

# **Data**

The dataset used in this analysis comes from the Diagnostic Wisconsin
Breast Cancer Database (WBCD), originally created from digitized images
of fine needle aspiration (FNA) biopsies taken from breast masses. The
version used here was downloaded from Kaggle, but the source dataset was
donated on October 31, 1995. Although imaging modalities have improved
significantly since 1995, modern systems continue to quantify similar
structural and morphological features, meaning that this dataset remains
an informative foundation for building a proof-of-concept modeling
framework that can later be extended to more advanced imaging
technologies.

Each observation corresponds to a single breast mass and contains 30
continuous predictors, derived from 10 underlying cell nucleus features:
radius, texture, perimeter, area, smoothness, compactness, concavity,
concave points, symmetry, fractal dimension. Each of these ten features
was recorded in three forms:

1.  Mean – the average value across the tumor sample
2.  Standard Error (SE) – the variability of that feature
3.  Worst (maximal) – the mean of the highest three values

The response variable is the diagnosis: malignant (M) or benign (B).

There are 569 observations, including 357 benign (62.7%) and 212
malignant (37.2%) cases. Although benign tumors are more common in the dataset, 
the dataset is only moderately imbalanced, so rebalancing procedures such as
SMOTE or class weighting are not necessary for this analysis.

The data contains no missing values, and all predictors were
standardized with four significant digits in the version we downloaded.
Because the features are continuous, structured, and well behaved, this
dataset is well suited for multiple modeling approaches including
logistic regression, LASSO, KNN, random forests, and decision trees.

Finally, this dataset provides a solid platform for exploratory data
analysis (EDA) and model interpretation because each predictor has a
clear biological meaning and can be reasoned about in clinical terms
(e.g., radius relates to tumor size, concavity relates to irregularity).

The 13 predictors selected from the LASSO logistic model can be grouped into three categories: those which measure mean, standard error, and worst. The mean variables include compactness and concave points. The standard error features are radius, texture, smoothness, compactness and fractal dimension. Lastly, the worst variables are radius, texure, smoothness, concavity, concave points and symmetry.

# **Modeling Methodology**

Our methodology consists of five main stages: exploratory data
analysis, threshold optimization and evaluation, preliminary model
comparison, regularized model selection, and nonlinear modeling with
generalized additive models (GAMs).

## Exploratory Data Analysis (EDA)

```{r, eval=TRUE}
# Loading the packages
library(tidyverse)
library(ggplot2)
library(dplyr)
library(class)
library(caret)
library(pROC)
library(glmnet)
library(GGally)
library(mgcv)
library(tidyr)
library(knitr)
library(kableExtra)
```

We began by conducting a comprehensive exploratory data analysis to
understand the distribution of each of the 30 predictors and to identify
any necessary transformations or outlier adjustments. Visualizations
included histograms, boxplots, and scatterplots across both classes
(benign vs. malignant).

Although certain variables showed skewed distributions, which is
expected in biological measurements, none raised concerns severe enough
to justify transformation. Outliers were examined but ultimately retained, 
as they reflect real biological variation rather than measurement error. 
No missing data were present, so no imputation or deletion procedures were 
required. The EDA thus served primarily to guide expectations about predictor 
behavior and to inform our choice of appropriate models.

```{r, results = 'asis', fig.cap="EDA for Interaction Terms", out.width="60%", fig.width=12, fig.height=4}
library(patchwork)
df <- read.csv("Data/data.csv")

# smoothness_worst vs concavity_worst
p1 <- ggplot(df, aes(smoothness_worst, concavity_worst, color = diagnosis)) +
  geom_point() +
  geom_smooth(method = "gam") + 
  ggtitle("Smoothness (Worst) and Concavity (Worst)") + 
  theme_bw()

# radius_worst vs concave.points_mean
p2 <- ggplot(df, aes(radius_worst, concave.points_mean, color = diagnosis)) +
  geom_point() +
  geom_smooth(method = "gam") + 
  ggtitle("Radius (Worst) and Concave Points (Worst)") + 
  theme_bw()

# fractal_dimension_se vs concavity_worst
p3 <- ggplot(df, aes(fractal_dimension_se, concavity_worst, color = diagnosis))+
  geom_point() +
  geom_smooth(method = "gam") + 
  ggtitle("Fractal Dimension (SE) and Concavity (Worst)") + 
  theme_bw()


(p1 | p2 | p3) + plot_layout(guides = "collect")
```

We also explored bivariate EDA to examine potential interaction effects between predictors. In Figure 1, there appears to be interaction effects between each of the listed pairs of predictors since the relationships between each pair differs with the benign and malignant tumors.

## Building a Model of a Mass

For one observation of the data, we have attempted to translate the variables into the model into a 3D representation of a mass. The purpose of this exercise was to gain intuition into how different variables translate into features of the mass.

** ABBY INSERT WORK HERE

## Cost Analysis and Threshold Selection 

We researched the cost of false positives and negatives to compute
a cost based loss function to determine the optimal classification
threshold. This threshold will reflect the asymmetric cost of false
negatives (missing a malignant tumor) versus false positives
(unnecessary further testing), and will guide our final decision rule
applied to our model. 

In our cost analysis, we have prioritized the care of the patient over everything. We are steadfast in our commitment to correctly reporting when cancer is present due to recovery being more possible when caught early. According to Dr. Grayson from University of Texas Medical Branch, a woman's psychological state determines her ability to be responsive to treatment. She emphasizes the urgency of diagnosis to ensure women are confident in the support and care they receive. She affirms that there is a mental cost in delaying treatment as the condition worsens and so do spirits of hope.

Additionally, we have found that patients who are able to receive their diagnosis earlier are subject to a more cost efficient treatment than a patients who receive their diagnosis in say stage IV. According to WebMD, women who are able to obtain care in the primary stages of cancer when the tumor is small and localized pay \$48,500 on average whereas a woman who obtains care during stage IV is likely to pay \$183,00.* (note that these are averages and insurance alleviation may differ from woman to woman) We prioritize this concern as 1 in 13 women report cancer treatment costs are higher than intitially anticipated, so much so that women also report avoiding going to the doctor in fear of having to pay an exuberant amount. Moreover, when cancer is caught late, it places a stressor on the hospital as well; in some cases, the hospital itself may not be equipped to handle an influx of patients with stage IV cancer but would be better equipped to eliminate cancer had it been caught earlier. Consequently, it is our goal to diagnosis tumor as malignant in women as soon as possible to optimize results and minimize financials in every domain.

As a result, our threshold selection prioritizes maximizing sensitivity to minimize false negatives. False-negative results can lead to delayed detection and treatment, resulting in greater long-term costs, morbidity, and mortality. One study estimated the national cost-savings in the U.S. from early diagnosis of cancers to be $26 billion per year due to reduced treatment intensity and improved outcomes (Brill, 2020). 

Prior research also shows that the accuracy of mammography screening increases with patient age, with sensitivity ranging from 76% to 86% and specificity ranging from 87% to 99% across age groups. (Newton, 2025). This gap in threshold shows why maximizing sensitivity matters: since detecting malignant tumors is the harder task, optimizing sensitivity is essential to improving screening effectiveness. Thus, our decision to choose a lower threshold is consistent with addressing current screening practices and reducing future costs associated with delayed diagnoses. 

## Preliminary Model Fitting

We next fit several classification models to evaluate baseline
predictive performance and to assess the trade-offs between accuracy,
interpretability, computational complexity, and clinical
trustworthiness. The models included: K-Nearest Neighbors (KNN), Random
Forest, LASSO, Standard Logistic Regression, and Decision Trees. Each
model was then evaluated using standard metrics including accuracy,
sensitivity, specificity, and confusion matrices. Cross validation was
used where appropriate to reduce sampling variability.

Although the random forest achieved strong predictive accuracy, it
lacked interpretability which should be prioritized since this model will be used by doctors to diagnose whether a breast cancer mass is benign or malignant at an early stage. 
KNN performed reasonably but does not provide coefficients, feature weights, or insight into mechanism. Standard logistic regression was interpretable but tended to overfit
without regularization due to the high number of correlated predictors.

## Model Selection and Rationale

Based on accuracy, interpretability, stability, and feature selection
capability, we selected the LASSO logistic regression model as our
primary predictive and inferential tool. LASSO automatically performs
variable selection by shrinking less important coefficients to zero,
providing a concise, clinically meaningful set of predictors and
potential interactions.

After selecting these top predictors, we further fit a GAM informed by
the LASSO selected features. We selected a GAM model for interpretability and flexibility in including nonlinear terms. GAM models are more interpretable than many other regression techniques since they enable visualization between features and the predicted malignancy probability with interpretability being essential for clinical decisions. Additionally, the nonlinearlity of GAMs is advantageous as tumor characteristics rarely relate to malignancy in linear ways, so GAMs can model predictors using a smooth function like splines to learn curves without assuming linearity which improves predictive performance. 

## Regularized Logistic Regression and Feature Selection

We first fit a LASSO logistic model to perform variable selection and establish a strong, interpretable baseline classifier. The diagnosis variable was
encoded as 1 for malignant and 0 for benign. Identifier columns were
removed, and the remaining predictors were standardized. The data were
randomly split into training (80%) and testing (20%) sets. A LASSO
logistic regression model was fit using 5-fold cross-validation to
select the optimal penalty parameter with `cv.glmnet`. This approach
addresses multicollinearity among predictors and prevents overfitting by
shrinking less informative coefficients to zero. The final LASSO logistic model
achieved approximately 99% accuracy on the held-out test set, with
perfect sensitivity and strong specificity. 

### Setting the Optimal Threshold
From the LASSO logistic regression, we determined the optimal threshold to predict whether a mass is benign or malignant based on the assigned probability. From the cost analysis and domain knowledge, we knew that the optimal threshold would maximize sensitivity. Using the `proc` library, we found that the optimal threshold to achieve perfect sensitivity is 0.351 using Youden's J statistic. Youden's method is a way to choose the "best" classification threshold by maximizing the combined performance of sensitiivty and specificity. The optimal threshold is one that maximizes J = Sensitivity + Specificity - 1. For this diagnostic classification problem, Youden's J statistic chooses the threshold that provides a balance weight to correctly identifying malignant and benign cases. Since cancer diagnosis requires higher sensitivity to avoid missing malignant cases, we want the threshold to be less than 0.5 so the false negative rate is decreased, so 0.351 seems a reasonable value. Other benefits for using Youden's statistics is that it is not distorted by how common cancer is in the dataset and it is transparet for clinicians. 

Although perfect sensitivity often comes at the cost of poor specificity and many false positives, we also observe a high specificity of 0.98 with perfect sensitivity. Hence, we are less concerned with the model classifying every observation as benign at this threshold given that the specificity is also very high. For the rest of the paper, 0.351 will be the threshold used for classifying a tumor. 

```{r}
df <- read.csv("Data/data.csv")
df <- df |> select(-X)
df <- data.frame(df)

# Adding a categorical variable for diagnosis
df$diagnosis <- ifelse(df$diagnosis == "M", 1, 0)
df <- df[ , !names(df) %in% c("id", "X", "Unnamed..32") ] # remove id columns

# Preparing the data for the train/test split
X <- model.matrix( ~ ., data = df)[, -1]
y <- df$diagnosis

# Train/test split
set.seed(123) 
n <- nrow(X)
train_idx <- sample(seq_len(n), size = 0.8 * n)

X_train <- X[train_idx, ][, -1]
X_test  <- X[-train_idx, ][, -1]
y_train <- y[train_idx]
y_test  <- y[-train_idx]

# Building the LASSO model
cvfit <- cv.glmnet(
  x = X_train,
  y = y_train,
  family = "binomial",
  alpha = 1,        # LASSO
  nfolds = 5        # 5-fold CV
)

# Predicted probabilities (logistic) at lambda.min
prob_test <- predict(cvfit, newx = X_test, 
                     s = "lambda.min", 
                     type = "response")
prob_test <- as.numeric(prob_test)

roc_obj <- roc(response = y_test, predictor = prob_test, positive = "1")

# Extracting the optimal threshold
coords <- coords(roc_obj,
       x = 1,
       input = "sensitivity",
       ret = c("threshold", "sensitivity", "specificity"))
opt_threshold <- coords$threshold

# Predicted class based on the optimal threshold
pred_class <- ifelse(prob_test >= opt_threshold, 1, 0)
```

```{r, eval=TRUE, results = 'asis', fig.cap="Figure 2: Confusion Matrix", out.width="80%"}
# Building the Confusion Matrix
cm <- table(Predicted = pred_class, Actual = y_test)
cm_df <- as.data.frame.matrix(cm)
cm_labeled <- data.frame(
  Outcome = c("Predicted Benign", "Predicted Malignant"),
  Benign  = cm[, "0"],
  Malignant = cm[, "1"]
)
kable(cm_labeled, caption = "Confusion Matrix", align = "c")
```
```{r, eval=FALSE}
accuracy <- mean(pred_class == y_test)
sens <- sum(pred_class == 1 & y_test == 1) / sum(y_test == 1)
spec <- sum(pred_class == 0 & y_test == 0) / sum(y_test == 0)
c(accuracy = accuracy, sensitivity = sens, specificity = spec)
```

### Model Diagnostics 
The Confusion Matrix above uses the optimal threshold and achieves perfect sensitivity, a specificity of 0.98, and a 99% accuracy. This indicates strong overall model performance and although there are possible concerns about model overfitting, we are confident in our results given that we used 5-fold CV to train the data and then tested the data on the hold-out set.  
```{r}
accuracy <- mean(pred_class == y_test)
sens <- sum(pred_class == 1 & y_test == 1) / sum(y_test == 1)
spec <- sum(pred_class == 0 & y_test == 0) / sum(y_test == 0)
#c(accuracy = accuracy, sensitivity = sens, specificity = spec)
```

Model performance was further evaluated using an ROC curve and a measure of AUC, providing a threshold-independent measure of discrimination. The AUC for the ROC curve is 0.9994 which is fairly high, giving us confidence in our model. 

The last step after building the LASSO logistic model was extracting the features which had non-zero coefficients for predicting the probability of a tumor being benign or malignant. Below is a list of the 13 features (out of 30) which the LASSO model indicated were important for predicting the probabilities ordered in descending order of magnitude. 
```{r, eval=TRUE}
# Extracting the coefficient features
coef_min <- coef(cvfit, s = "lambda.min")
coef_df <- as.data.frame(as.matrix(coef_min))
coef_df$feature <- rownames(coef_df)
names(coef_df)[1] <- "coefficient"

# Filtering for coefficients which aren't 0
feature_list <- coef_df |>
  filter(feature != "(Intercept)") |>
  filter(coefficient != 0) |>
  arrange(desc(abs(coefficient))) |>
  pull(feature)

# Grouping by the values they end in
group_se    <- feature_list[grepl("_se$", feature_list)]
group_mean  <- feature_list[grepl("_mean$", feature_list)]
group_worst <- feature_list[grepl("_worst$", feature_list)]

# Determine longest group
max_len <- max(length(group_se), length(group_mean), length(group_worst))

# Pad groups to same length
group_se_padded    <- c(group_se,    rep("", max_len - length(group_se)))
group_mean_padded  <- c(group_mean,  rep("", max_len - length(group_mean)))
group_worst_padded <- c(group_worst, rep("", max_len - length(group_worst)))

# Build final data frame
df_grouped <- data.frame(
  Mean  = group_mean_padded,
  SE    = group_se_padded,
  Worst = group_worst_padded
)

kable(
  df_grouped,
  caption = "Selected Features",
  align = "c"
)
```

In summary, the LASSO logistic model yielded a parsimonious subset of clinically
interpretable predictors, which served two purposes:

1.  Establishing a transparent, high-performing baseline classifier.
2.  Guiding subsequent nonlinear modeling by identifying variables most
    strongly associated with malignancy.

## Nonlinearity Assessment and Motivation for GAMs

While LASSO logistic regression provides interpretability through
coefficients, it assumes linear relationships between predictors and the
log-odds of malignancy. To assess whether this assumption was
appropriate, we conducted a systematic investigation of potential
nonlinearity.

For each predictor selected by the LASSO model, we binned the predictor
into quantiles, computed observed malignancy proportions within each
bin, and examined both probability-scale and log-odds-scale plots.
Several predictors, most notably texture_se, compactness_se,
fractal_dimension_se, texture_worst and concavity_worst, exhibited clear nonlinear patterns,
suggesting that a purely linear model may be misspecified.

## Generalized Additive Models (GAMs)

To flexibly model these nonlinear effects while preserving
interpretability, we employed Generalized Additive Models (GAMs) with a
binomial logit link. GAMs model the log-odds of malignancy as a sum of
smooth functions of predictors, allowing nonlinear relationships to be
captured without requiring explicit interaction terms.

We compared different spline bases (thin-plate regression splines,
cubic regression splines, and thin-plate shrinkage splines) for individual predictors using AIC, REML
scores, and visual inspection. Restricted Maximum Likelihood (REML) was used to compare the splines instead of Maximum Likelihood because REML provides less biased estimates for variance components. Although no spline performed definitively better across all metrics, cubic splines generally had lower AIC and REML across variables. Additionally, cubic splines had a lower range of plausible predicted probabilities and less wiggly than the other two splines. Thus, we chose to use cubic regression splines since consistently
provided better stability and more realistic probability behavior, and
were therefore selected.

After determining the appropriate spline to be cubic splines, we selected which features show strong violation of nonlinearity so we can include them in splines. All other variables which don't show strong evidence of nonlinearity will be included as main effects or interaction terms. To do this, we first binned each of the predictors and calculated the appropriate log-odds of malignancy within the training data. We then visualized the logit plot for each predictor to determine where we could see strong violations of nonlinearity. 

```{r}
# Getting the training dataframe
X2 <- model.matrix( ~ ., data = df)[, -1]

# Getting the dataframes
train_df <- as.data.frame(X2[train_idx, ])
test_df <- as.data.frame(X2[-train_idx, ])
```

```{r, eval = TRUE, results = 'asis', fig.cap="Log-Odds Plots", out.width="80%", fig.width=12}
nbins <- 20
yvar <- "diagnosis"       # your binary outcome

# Function that bins & plots one variable
make_binned_plots <- function(df, xvar, yvar, nbins = 20) {
  
  # 1) Compute bins
  binned <- df %>%
    mutate(bin = ntile(.data[[xvar]], nbins)) %>%
    group_by(bin) %>%
    summarise(
      x_mid = mean(.data[[xvar]], na.rm = TRUE),
      n     = n(),
      prop  = mean(.data[[yvar]], na.rm = TRUE),
      .groups = "drop"
    ) %>%
    mutate(
      se    = sqrt(prop * (1 - prop) / pmax(n, 1)),
      lower = pmax(0, prop - 1.96 * se),
      upper = pmin(1, prop + 1.96 * se),
      eps   = 1e-3,
      logit = log((prop + eps) / (1 - prop + eps))
    )
  
  # 2) Proportion plot
  p_prop <- ggplot(binned, aes(x = x_mid, y = prop)) +
    geom_point(size = 2) +
    geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.01) +
    labs(
      title = paste("Binned proportion for", xvar),
      x = xvar,
      y = "Observed proportion"
    ) +
    theme_minimal()
  
  # 3) Logit plot
  p_logit <- ggplot(binned, aes(x = x_mid, y = logit)) +
    geom_point(size = 2) +
    geom_smooth(method = "loess", se = TRUE) +
    labs(,
      x = xvar,
      y = "Estimated log-odds"
    ) +
    theme_minimal()
  
  return(list(binned = binned, prop_plot = p_prop, logit_plot = p_logit))
}

# ---- Loop over all predictors ----
results <- map(feature_list, ~ make_binned_plots(train_df, .x, yvar))
names(results) <- feature_list

## Showing all of the plots
p1 <- results[["smoothness_se"]]$logit_plot
p2 <- results[["concave.points_mean"]]$logit_plot
p3 <- results[["radius_se"]]$logit_plot
p4 <- results[["radius_worst"]]$logit_plot
p5 <- results[["texture_worst"]]$logit_plot
p6 <- results[["smoothness_worst"]]$logit_plot
p7 <- results[["concavity_worst"]]$logit_plot
p8 <- results[["concave.points_worst"]]$logit_plot
p9 <- results[["symmetry_worst"]]$logit_plot
p10 <- results[["compactness_mean"]]$logit_plot
p11 <- results[["texture_se"]]$logit_plot
p12 <- results[["compactness_se"]]$logit_plot
p13 <- results[["fractal_dimension_se"]]$logit_plot

# Arranging the plots
plots <- list(
  p1, p2, p3, p4, p5, p6,
  p7, p8, p9, p10, p11, p12, p13
)

wrap_plots(plots, ncol = 3) & 
  theme(legend.position = "bottom")

```

In Figure 3, we observe that many of the variables to violate nonlinearity. Thus, we also want to quantitatively determine which features should be included as splines by comparing a GLM model to predict malignancy probability with a GAM using only a cubic spline with the selected feature. Then, we compared the change in AIC between the GLM and GAM as well as the p-value for the Chi-Squared test where the cutoff was set at $\alpha = 0.01$. Features which had a drop in AIC with a magnitude of at least 4 and a significant p-value were identified as features with strong evidence of non-linearity. The delta AIC value of 4 was selected using the convention that a drop in AIC greater than 4 is evidence that the GAM meaningfully improves the model fit. 

```{r, eval=TRUE, results = 'asis', fig.cap="Features with Strong Nonlinearity", out.width="80%"}
results <- map_dfr(feature_list, function(var) {
  
  # build formulas
  form_glm <- as.formula(paste("diagnosis ~", var))
  form_gam <- as.formula(paste("diagnosis ~ s(", var, ", k=10)", sep = ""))
  
  # fit models
  mod_glm <- glm(form_glm, data=train_df, family = binomial("logit"))
  mod_gam <- gam(form_gam, data=train_df, bs="cr",
                 family = binomial("logit"), method="REML")
  
  # AIC comparison
  aic_vals <- AIC(mod_glm, mod_gam)
  
  # Likelihood ratio test (nested model test)
  an <- anova(mod_glm, mod_gam, test="Chisq")
  pval <- an$`Pr(>Chi)`[2]   # GAM vs GLM

  
  tibble(
    variable = var,
    glm_AIC  = aic_vals$AIC[1],
    gam_AIC  = aic_vals$AIC[2],
    delta_AIC = aic_vals$AIC[1] - aic_vals$AIC[2],
    LRT_pvalue = an$`Pr(>Chi)`[2],  # the p-value comparing GLM vs GAM
    significant_0.01 = ifelse(!is.na(pval) & pval < 0.01, "YES", "NO")

  )
})
results |> 
  filter(abs(delta_AIC) > 4, significant_0.01 == "YES") |>
  select(variable, delta_AIC) |>
  kable()

```

From the results above, the following features were included in the GAM as cubic splines: texture_se, compactness_se, fractal_dimension_se, texture_worst, and concavity_worst. 

From here, the initial GAM model was built using cubic splines for 5 nonlinear features and including the other 8 features as main effects. 
```{r, eval=TRUE, results = 'asis', fig.cap="Figure 5: Confusion Matrix for Model without Interactions", out.width="80%"}
formula_all <- diagnosis ~ 
  s(texture_se, bs = "cr", k = 10) +
  s(compactness_se, bs = "cr", k = 10) +
  s(fractal_dimension_se, bs = "cr", k = 10) +
  concave.points_mean + radius_se + radius_worst +
  s(texture_worst, bs = "cr", k = 10) +
  s(concavity_worst, bs = "cr", k = 10) + 
  compactness_mean  + concave.points_mean + radius_se + 
  smoothness_se + radius_worst + smoothness_worst + concave.points_worst + 
  symmetry_worst

mod_all <- bam(formula_all,
               data = train_df,
               family = binomial(link = "logit"),
               method = "fREML",
               discrete = TRUE,      # huge speed boost
               select = TRUE)

df_test_pred <- test_df |>
  mutate(
    .pred_prob  = predict(mod_all, newdata = test_df, type = "response"),
    .pred_link  = predict(mod_all, newdata = test_df, type = "link"),
    .pred_class = as.integer(.pred_prob >= opt_threshold),
    .obs        = as.integer(!!rlang::sym("diagnosis"))
  )

cm <- caret::confusionMatrix(
  factor(df_test_pred$.pred_class, levels = c(0,1)),
  factor(df_test_pred$.obs, levels = c(0,1)),
  positive = "1"
)

#cm$overall["Accuracy"]
#cm$byClass[c("Sensitivity", "Specificity")]

cm2 <- table(Predicted = df_test_pred$.pred_class, Actual = df_test_pred$.obs)
cm_df2 <- as.data.frame.matrix(cm2)
cm_labeled2 <- data.frame(
  Outcome = c("Predicted Benign", "Predicted Malignant"),
  Benign  = cm2[, "0"],
  Malignant = cm2[, "1"]
)
kable(cm_labeled2, caption = "Confusion Matrix", align = "c")

roc_obj <- roc(df_test_pred$.obs, df_test_pred$.pred_prob, quiet = TRUE)
auc_val <- as.numeric(pROC::auc(roc_obj))
#print(auc_val)
```

The model performance for this base GAM was assessed using the test dataset. The model has a 98% accuracy as well as a sensitivity of 0.981 and a specificity of 0.983. The model also has an area under the curve extremely close to 1 indicating the model has a strong ability to distinguish between classes. For cancer diagnoses we want to maximize sensitivity, so we next consider adding interactions to improve our model to further increase sensitivity. 

```{r, eval=FALSE}
summary(mod_all)
```

** ADD INTERPRETATION HERE


## Adding Interaction Effects

Although GAMs capture nonlinear marginal effects, additive models may
miss important interactions. Guided by domain knowledge and prior
findings on the Wisconsin Breast Cancer Dataset, we extended the
additive GAM to include tensor-product interaction terms between
clinically meaningful feature pairs like tumor size and boundary
irregularity. Candidate interactions included: smoothness × concavity,
radius × concave points, and fractal dimension × concavity.

Justification for these interactions are as follows:
1. smoothness_worst x concavity_worst: Low smoothness (coarse boundary) and high concavity is a malignancy marker. There are different risks associated with smooth but concave masses compared to rough but concave ones. 
2. radius_worst x concave.points_mean: Concave points measure sharp inward curves on the tumor boundary whereas radius measures size. Large lesions with many concave points are signals of malignancy.
3. fractal_dimension_se x concavity_worst: Fractal dimension relates to edge complexity and high fractal dimension with strong concavity could indicate irregular growth.  

These interactions were implemented using tensor-product smooths to
accommodate differing scales and nonlinear dependence. Model comparison
using AIC and likelihood ratio tests demonstrated a statistically
significant improvement in fit over the purely additive GAM. We fit several different models with different interaction terms, but the final model was selected for having the largest drop in AIC as well as a significant p-value from the Chi-Squared test. 

```{r, eval=TRUE}
# Building the formula with these interactions
formula_best <- diagnosis ~ 
  s(texture_se, bs = "cr", k = 10) +
  s(compactness_se, bs = "cr", k = 10) +
  s(fractal_dimension_se, bs = "cr", k = 10) +
  concave.points_mean + radius_se + radius_worst +
  s(texture_worst, bs = "cr", k = 10) +
  s(concavity_worst, bs = "cr", k = 10) + 
  compactness_mean  + concave.points_mean + radius_se + 
  smoothness_se + radius_worst + smoothness_worst + concave.points_worst + 
  symmetry_worst + 
  ti(concave.points_mean, radius_worst, bs=c("cr", "cr")) + 
  ti(concavity_worst, smoothness_worst, bs=c("cr", "cr")) +
  ti(concavity_worst, fractal_dimension_se, bs=c("cr", "cr")) 


# Building the best model
mod_best <- bam(formula_best,
               data = train_df,
               family = binomial(link = "logit"),
               method = "fREML",
               discrete = TRUE,      # huge speed boost
               select = TRUE)

#AIC(mod_all, mod_best)
#anova(mod_all, mod_best, test="Chisq")
```
The final p-value from the Chi-squared test for the model with interactions compared to the model without was 0.002 which is significant past the $\alpha = 0.01$ level. 

We then assessed the model performance of this GAM with interactions on the test dataset.
```{r, eval=TRUE, results = 'asis', fig.cap="Figure 6: Confusion Matrix for Model with Interactions", out.width="80%"}
df_test_pred_best <- test_df |>
  mutate(
    .pred_prob  = predict(mod_best, newdata = test_df, type = "response"),
    .pred_link  = predict(mod_best, newdata = test_df, type = "link"),
    .pred_class = as.integer(.pred_prob >= opt_threshold),
    .obs        = as.integer(!!rlang::sym("diagnosis"))
  )

cm2_best <- caret::confusionMatrix(
  factor(df_test_pred_best$.pred_class, levels = c(0,1)),
  factor(df_test_pred_best$.obs, levels = c(0,1)),
  positive = "1"
)

#cm2_best$overall["Accuracy"]
#cm2_best$byClass[c("Sensitivity", "Specificity")]

cm2_best <- table(Predicted = df_test_pred_best$.pred_class, 
             Actual = df_test_pred_best$.obs)
cm_df2_best <- as.data.frame.matrix(cm2_best)
cm_labeled2_best <- data.frame(
  Outcome = c("Predicted Benign", "Predicted Malignant"),
  Benign  = cm2_best[, "0"],
  Malignant = cm2_best[, "1"]
)
kable(cm_labeled2_best, caption = "Confusion Matrix", align = "c")

roc_obj_best <- roc(df_test_pred_best$.obs, 
               df_test_pred_best$.pred_prob, quiet = TRUE)
auc_val_best <- as.numeric(pROC::auc(roc_obj))
#print(auc_val_best)
```
We observe that the Confusion matrix is identical to the GAM without interactions, but this is likely due to the small sample size. Additionally, the accuracy, sensitivity, and specificity remain the same but this can likely also be attributed to the small sample size and the strong performance of the original model without interactions.

```{r, eval = FALSE}
summary(mod_best)
```

** ADD INTERPRETATION HERE

## Final Model Selection

The GAM with selected nonlinear terms and domain-motivated interactions
was chosen as the final model, as it achieved strong predictive
performance (high AUC and balanced sensitivity/specificity), captured
clinically plausible nonlinear and interaction effects, remained
interpretable through smooth and interaction visualizations, and aligned
with the project’s emphasis on transparency and trust in medical
decision making. The final sensitivity was 0.981 which is very high since 98.1% of malignancy cases were correctly identified in the model, so only 1.9% of cases are false negatives which we want to minimize. 

## Downstream Uses of the Model

Our final predictive model can serve several downstream purposes.
Clinicians may use a model derived malignancy probability as a
preliminary triage tool to help determine urgency of follow up imaging
or need for biopsy. Researchers may use our inferred feature importance
patterns to better understand structural characteristics of malignant
tissue, signaling which measurements warrant closer attention in future
imaging protocols. In healthcare settings, such tools can also guide
resource allocation by highlighting high risk cases sooner, especially
when imaging workloads are high (with over 42 million mammograms
performed annually across the US and UK, Nature 2020).

By ensuring that our model is interpretable and clearly justified, it
can be used ethically and effectively in supporting clinical workflows,
improving early detection, and helping reduce false positives which have
been long recognized as a challenge in mammographic screening.

# **Results**

## Key Findings

Since both our final GAM models have essentially identical predictive results, we have two potential models for clinicians to choose between. If they want a simpler model with fewer features, the GAM without interactions would be a great predictive model to use. On the other hand if they want to prioritize more complex models to explain ways that certain features interact, they should use the GAM with interactions.

Although the LASSO logistic model wasn't selected as our final model for predictions, the features it identified as key features are very helpful for understanding which features of a mass are most indicative of the mass' probability of being malignant. Identifying these features allows clinicians to grasp which aspects of the mass they should focus on when they receive the data by seeing if any of these selected 13 features have values outside of a normal range.

## Limitations

One limitation is that the data is from 1995. While our work provides proof of concept modeling, future work can expand upon these findings by utilizing modern imaging technology (3D mammography, MRI, MBI) that can can detect additional features. Additionally, we are working with a fairly small sample size of observations (569 total). We would expect to see better predictive accuracy and improvement in sensitivity when adding interactions if we had a larger test dataset.  

Another limitation is that the predictors are based on computed summary statistics for each tumor (mean, SE, "worst"). Although having these values provide general trends, they may fail to capture specific nuances to tumor characteristics compared to using raw imaging data. An additional limitation to the models selected, lasso logistic regression and GAMs, is that important nonlinear or interaction effects may be overlooked, since regression relies on linear assumption and GAMs may fail to capture complex, higher-order interactions between tumor features. 

# **Conclusion**

## Future Analysis

Further analysis in the cancer field is pivotal as healthcare advancements are vast and rapidy growinf to fulfill the needs of patients. With that, we have prioritized recommendations on how to best optimize future research on Breast Cancer Diagnosis. We suggest investigating the interaction effects between stages of cancer and existing variables. We affirm that this insight would provide important information to healthcare professionals when it comes to assessing urgency in diagnosis. According to the CDC, most breast cancer diagnosis are caught in the early stages, but when caught in a later stage, the cancer has likely spread to other organs which would coincide with the results we gathered from our model (ie, as radius increases so does the probability of cancer). 

Another suggestion for future work would be to include age, race/ethnicity and socioeconomic class in the data. Not only would this allow us to see how we can ensure proper representation, but also, according to the American Cancer Society, these three factors actually play a role in what stage of cancer (and thus the probability the tumor is malignant or begin) the diagnosis is received. 

Our findings have crafted the framework for future researchers to continuously test our model with recent data, and we encourage others to expand on our results with suggestions above or beyond.

# **References**
Github: https://github.com/gradypurcell/STA-325-Final-Project.git  

# ** Sources**
CDC: https://www.cdc.gov/united-states-cancer-statistics/publications/breast-cancer-stat-bite.html
American Cancer Society: https://www.cancer.org/content/dam/cancer-org/research/cancer-facts-and-statistics/breast-cancer-facts-and-figures/2022-2024-breast-cancer-fact-figures-acs.pdf
WebMD: https://www.webmd.com/breast-cancer/breast-cancer-costs
UTMB: https://www.utmb.edu/news/article/health-blog/2025/10/20/beyond-the-diagnosis--facing-the-emotional-side-of-breast-cancer