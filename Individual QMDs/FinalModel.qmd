# QMD for the Final Model

## Reading in the data and packages
```{r}
# Loading the packages
library(tidyverse)
library(ggplot2)
library(dplyr)
library(class)
library(caret)
library(pROC)
library(glmnet)
library(GGally)
library(mgcv)
library(tidyr)
library(knitr)
library(kableExtra)

```

Loading the data
```{r}
# Loading the data
df <- read.csv("~/STA-325-Final-Project/Data/data.csv")
df <- df |> select(-X)
df <- data.frame(df)

# Adding a categorical variable for diagnosis
df$diagnosis <- ifelse(df$diagnosis == "M", 1, 0)
df <- df[ , !names(df) %in% c("id", "X", "Unnamed..32") ] # remove id columns

```

## Lasso Logistic Model
```{r}
# Preparing the data for the train/test split
X <- model.matrix( ~ ., data = df)[, -1]
y <- df$diagnosis

# Train/test split
set.seed(123) 
n <- nrow(X)
train_idx <- sample(seq_len(n), size = 0.8 * n)

X_train <- X[train_idx, ][, -1]
X_test  <- X[-train_idx, ][, -1]
y_train <- y[train_idx]
y_test  <- y[-train_idx]
```

Building the Lasso model using 5-fold cross validation. 
```{r}
cvfit <- cv.glmnet(
  x = X_train,
  y = y_train,
  family = "binomial",
  alpha = 1,        # LASSO
  nfolds = 5        # 5-fold CV
)

```

Model diagnostics for the lasso model.
```{r}
plot(cvfit) 
print(cvfit$lambda.min)
```

Extracting the coefficients from the Lasso model.
```{r}
# Extracting the coefficient features
coef_min <- coef(cvfit, s = "lambda.min")
coef_df <- as.data.frame(as.matrix(coef_min))
coef_df$feature <- rownames(coef_df)
names(coef_df)[1] <- "coefficient"

# Filtering for coefficients which aren't 0
feature_list <- coef_df |>
  filter(feature != "(Intercept)") |> 
  filter(coefficient != 0) |>
  pull(feature)
```
### Optimal Threshold
Since we are working on cancer detection, we want to minimize the false negative rate since we would prefer to incorrectly classify a tumor as malignant. In order to to this, we want to maximize sensitivity. Since sensitivity is already fairly high when we have a threshold at 0.5, we are going to find the threshold which gives us a maximal sensitivity level of 1.
```{r}
roc_obj <- roc(response = y_test, predictor = prob_test, positive = "1")
coords <- coords(roc_obj,
       x = 1,
       input = "sensitivity",
       ret = c("threshold", "sensitivity", "specificity"))
coords 
```

Setting the optimal threshold which falls within a range of outside research and is extracted from the code above. 
```{r}
# This is the variable that changes based on the threshold
opt_threshold <- coords$threshold

```

### Model Diagnostics
Getting the predicted probabilities based on the optimal threshold and minimal lambda value.
```{r}
# Predicted probabilities (logistic) at lambda.min
prob_test <- predict(cvfit, newx = X_test, 
                     s = "lambda.min", 
                     type = "response")
prob_test <- as.numeric(prob_test)

# Predicted class based on the optimal threshold
pred_class <- ifelse(prob_test >= opt_threshold, 1, 0)
```

Building the confusion matrix based on the predicted probabilities. We see below that there are no false negatives which is expected given the threshold we set maximizes sensitivity. 
```{r}
cm <- table(Predicted = pred_class, Actual = y_test)

cm_df <- as.data.frame.matrix(cm)

cm_labeled <- data.frame(
  Outcome = c("Predicted Benign", "Predicted Malignant"),
  Benign  = cm[, "0"],
  Malignant = cm[, "1"]
)

kable(cm_labeled, caption = "Confusion Matrix", align = "c")

```

Gathering other metrics for the accuracy, sensitivity, and specificity. 
```{r}
accuracy <- mean(pred_class == y_test)
sens <- sum(pred_class == 1 & y_test == 1) / sum(y_test == 1)
spec <- sum(pred_class == 0 & y_test == 0) / sum(y_test == 0)
c(accuracy = accuracy, sensitivity = sens, specificity = spec)

```

Plotting an AUC curve to assess model performance. 
```{r}
roc_obj <- roc(response = y_test, predictor = prob_test, positive = "1")
auc_val <- auc(roc_obj)

print(auc_val)
plot(roc_obj, main = paste("AUC =", round(auc_val, 3)),
     xlab = "1 - Specificity",
     legacy.axes=TRUE)
```
Based on the model performance on the test data above, the model has high accuracy as well as sensitivity and specificity. 

### Final Features
The below features are the ones that the lasso logistic model kept in the model, so we will use these features for building the GAM model.
```{r}
print(feature_list)
```

## GAM Model
Using the same training indices from the lasso logistic model but including the diagnosis. 
```{r}
# Getting the training dataframe
X2 <- model.matrix( ~ ., data = df)[, -1]

# Getting the dataframes
train_df <- as.data.frame(X2[train_idx, ])
test_df <- as.data.frame(X2[-train_idx])
```

### Deciding Splines to Use
Using a variable which we know is in the final spline model (from the analysis below), we will test the predictive performance across three different types of splines:
- Thin-Plate spline: Default type of spline with locations which are automatically choose. TP is oftne used for nonlinear relationships with minimal tuning. TP splines have an optimal smooth basis and are very smooth when considering interpretation. Performs well for high dimensional / multi-variate data. 
- Cubic spline: Uses a fixed set of knots to create local piecewise polynomial fits. Cubic splines can be less interepretable with more local wiggles.
- Thin Plate Shrinkage: Thin-Plate splines with extra penalty. 

The method used is Restricted Maximum Likelihood (REML) which is a statistical technique for fitting linear models and estimating variance. REML is preferred over Maximum Likelihood because it provides less biased estimates for variance components.


```{r}
# Building the different models
## Thin-Plate Splines
m_tp  <- gam(diagnosis ~ s(smoothness_se, bs = "tp", 
                           k = 10), 
            data = train_df,
            family = binomial(link = "logit"), 
            method = "REML")

## Cubic Splines
m_cr  <- gam(diagnosis ~ s(smoothness_se, bs = "cr", 
                           k = 10), 
            data = train_df,
            family = binomial(link = "logit"), 
            method = "REML")

## Thin-Plate Shrinkage
m_tp_sel  <- gam(diagnosis ~ s(smoothness_se, bs = "tp", 
                           k = 10), 
            data = train_df,
            family = binomial(link = "logit"), 
            method = "REML",
            select = TRUE)
```

Comparing the different types of splines.
```{r}
models <- list(tp = m_tp, cr = m_cr, tp_sel = m_tp_sel)
compare_df <- tibble(
  model = names(models),
  edf = map_dbl(models, ~ sum(summary(.x)$s.table[ , "edf"], na.rm = TRUE)),
  logLik = map_dbl(models, logLik),
  AIC = map_dbl(models, AIC),
  REML = map_dbl(models, ~ .x$gcv.ubre) # optional measure stored in object
)
print(compare_df)
```

Overall, the thin-plate spline has the lowest log-likelihood as well as AIC. However, the cubic spline has the lowest REML. Given that these findings contradict each other it is unclear which type of spline is best. Thus, we will select cubic splines because we are more familiar with cubic splines as piecewise cubic polynimals with knots. They are also computationally cheap and work well for simple 1D smooths as they are stable and predictable. Additionally, we are prioritizing interpretable models and cubic splines are preferred for interpretable 1D smooths. 

To support our findings, we also built a visual to compare the fit of these models across different ranges of smoothness_se.
```{r}
#prepare grid across smoothness_se
grid <- tibble(smoothness_se = seq(min(train_df$smoothness_se, na.rm = TRUE),
                            max(train_df$smoothness_se, na.rm = TRUE),
                            length.out = 200))

preds <- map_dfr(models, function(mod) {
  p_link <- predict(mod, newdata = grid, type = "link", 
                    se.fit = TRUE)
  tibble(
    smoothness_se = grid$smoothness_se,
    model = deparse(substitute(mod)),  # we'll replace model name next
    fit_link = p_link$fit,
    se_link  = p_link$se.fit,
    fit_resp = plogis(p_link$fit),                 # inverse logit -> predicted probability
    upr = plogis(p_link$fit + 2 * p_link$se.fit),
    lwr = plogis(p_link$fit - 2 * p_link$se.fit)
  )
}, .id = "model_id")

# fix model names: .id currently 1..n, replace with real names
preds$model <- rep(names(models), each = nrow(grid))

# Plot predicted probability (response scale)
ggplot(preds, aes(x = smoothness_se, y = fit_resp, color = model)) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr, fill = model), alpha = 0.15, color = NA) +
  theme_minimal() +
  labs(y = "Predicted probability", title = "GAM spline basis comparison (binomial, logit link)")

# If you want to examine the logit (link) scale instead:
ggplot(preds, aes(x = smoothness_se, y = fit_link, color = model)) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = fit_link - 2*se_link, ymax = fit_link + 2*se_link, fill = model),
              alpha = 0.12, color = NA) +
  labs(y = "Fitted logit (link)", title = "Linear predictor (logit) across smoothness_se")
```

From the visual above, it is clear that the thin-plate shrinkage spline does not perform the best given that the predicted probability does not change very much as the smoothness varies. 

### Deciding Which Variables To Include in Splines
```{r}
results <- map_dfr(feature_list, function(var) {
  
  # build formulas
  form_glm <- as.formula(paste("diagnosis ~", var))
  form_gam <- as.formula(paste("diagnosis ~ s(", var, ", k=10)", sep = ""))
  
  # fit models
  mod_glm <- glm(form_glm, data=X_train2, family = binomial("logit"))
  mod_gam <- gam(form_gam, data=X_train2, bs="cr",
                 family = binomial("logit"), method="REML")
  
  # AIC comparison
  aic_vals <- AIC(mod_glm, mod_gam)
  
  # Likelihood ratio test (nested model test)
  an <- anova(mod_glm, mod_gam, test="Chisq")
  pval <- an$`Pr(>Chi)`[2]   # GAM vs GLM

  
  tibble(
    variable = var,
    glm_AIC  = aic_vals$AIC[1],
    gam_AIC  = aic_vals$AIC[2],
    delta_AIC = aic_vals$AIC[1] - aic_vals$AIC[2],
    LRT_pvalue = an$`Pr(>Chi)`[2],  # the p-value comparing GLM vs GAM
    significant_0.01 = ifelse(!is.na(pval) & pval < 0.01, "YES", "NO")

  )
})
```




