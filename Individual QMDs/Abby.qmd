
```{r}
library(here)
library(glmnet)
library(dplyr)

data <- read.csv(here("Data", "data.csv"))

# remove column X - irrelevant to analyses
data <- data %>% select(-X)

df <- data.frame(data)

```

## Lasso-Penalized Logistic Regression

```{r}
# 1 = malignant, 0 = benign
df$diagnosis <- ifelse(df$diagnosis == "M", 1, 0)

log_model <- glm(diagnosis ~ ., data = df, family = binomial)

# summary(log_model)
# using all 30 predictors -> multicollinearity

# instead use LASSO regression to actually perform variable selection 

df <- df[ , !names(df) %in% c("id", "X", "Unnamed..32") ] # remove id columns

X <- model.matrix(diagnosis ~ ., data = df)[, -1]
y <- df$diagnosis

set.seed(123) 
n <- nrow(X)
train_idx <- sample(seq_len(n), size = 0.8 * n)

X_train <- X[train_idx, ]
X_test  <- X[-train_idx, ]
y_train <- y[train_idx]
y_test  <- y[-train_idx]

cvfit <- cv.glmnet(
  x = X_train,
  y = y_train,
  family = "binomial",
  alpha = 1,        # LASSO
  nfolds = 5        # 5-fold CV
)

plot(cvfit) 

prob_test <- predict(cvfit, newx = X_test, s = "lambda.min", type = "response")

# Turn into class predictions
pred_test <- ifelse(prob_test > 0.5, 1, 0)

# evaluate diff thresholds to reduce chances of false negatives
# threshold <- 0.3
# pred_test_adj <- ifelse(prob_test > threshold, 1, 0)

# table(pred_test_adj, y_test)

# Accuracy
mean(pred_test == y_test)

# 98% accuracy on test data

```


```{r}
# determining thresholds

library(caret)

evaluate_threshold <- function(prob, truth, threshold) {
  pred <- ifelse(prob > threshold, 1, 0)
  cm <- confusionMatrix(
    factor(pred, levels = c(0,1)),
    factor(truth, levels = c(0,1)),
    mode = "everything"
  )
  tibble(
    threshold = threshold,
    accuracy = cm$overall["Accuracy"],
    sensitivity = cm$byClass["Sensitivity"],   # True Positive Rate
    specificity = cm$byClass["Specificity"],   # True Negative Rate
    fn = cm$table["1","0"],  # False negatives: predicted benign, truth malignant
    fp = cm$table["0","1"]   # False positives
  )
}

thresholds <- seq(0.05, 0.50, by = 0.01)

results <- bind_rows(
  lapply(thresholds, evaluate_threshold, prob = prob_test, truth = y_test)
)

print(results)

best <- results %>% filter(sensitivity >= 0.95) %>% slice(1)
best
```
Most ideal threshold = 0.19
Still maintains an accuracy of 97% with sensitivity of 95%

```{r}
threshold <- 0.19
pred_test_adj <- ifelse(prob_test > threshold, 1, 0)

confusionMatrix(
  factor(pred_test_adj, levels = c(0,1)),
  factor(y_test, levels = c(0,1)),
  mode = "everything"
)
```


```{r}
# showing which predictors are the most important

coef(cvfit, s = "lambda.min")

# computing the confusion matrix

table(pred_test, y_test)
```

Based on the LASSO regression, the model predicted 1 false negative: gave 0 (benign) when the actual was malignant (1). 

Predictors that are the most telling: 
The strongest ones that increase the log odds of a tumor being malignant- 
- concave points mean, smoothness_se
The strongest ones that decrease the log odds of tumor being malignant (more likely to be benign)
- compactness mean, fractal dimension se, compactness se

Malignant tumors tend to have more irregular, concave, asymmetric nuclei; worse ones have more abnormalities and there's high variation 
Benign tumors tend to be more uniform and have lower fractal dimension variability

## EDA for Lasso Regression Model

```{r}
library(ggplot2)
library(tidyverse)

df_long <- df %>%
  pivot_longer(cols = -diagnosis, names_to = "variable", values_to = "value")

ggplot(df_long, aes(value)) +
  geom_histogram(bins = 30, fill = "gray70", color = "black") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Distribution of Predictor Variables") +
  theme_minimal()

```


```{r}
library(corrplot)
corr_matrix <- cor(df %>% select(-diagnosis))

corrplot(corr_matrix,
         method = "color",
         tl.cex = 0.6,
         number.cex = 0.5,
         title = "Correlation Heatmap of Predictors",
         mar=c(0,0,2,0))
```

Interpretation of this plot: Strong patterns of correlation and multicollinearity in the data. As seen in the top left block for "mean", the variables (radius_mean, texture_mean, perimeter_mean, area_mean, compactness_mean, concavity_mean, concave.points_mean) are strongly correlated b/n 0.7-0.9 as indicated by the dark blue color. Lasso will probably select only a few predictors from this cluster and shrink the rest of the coefficients to zero due to redudancy. For "se", seem to be moderately correlated. For "worst", strongest correlations appear near the bottom right for variables radius_worst, area_worst, concavity, concave.points, and perimeter. 



## Classification Trees

```{r}
library(rpart)

df2 <- read.csv("data.csv")

# 2. Remove ID and empty column
df2 <- df2[ , !names(df) %in% c("id", "Unnamed..32") ]


df$diagnosis <- factor(df2$diagnosis, levels = c("B", "M"))

```

```{r}
set.seed(123)
n <- nrow(df)
train_idx <- sample(seq_len(n), size = 0.8 * n)

train <- df[train_idx, ]
test  <- df[-train_idx, ]

tree_model <- rpart(
  diagnosis ~ .,
  data = train,
  method = "class",
  control = rpart.control(cp = 0.01)   # complexity parameter
)

# evaluate accuracy of tree on testing data

pred <- predict(tree_model, newdata = test, type = "class")

cm <- table(Predicted = pred, Actual = test$diagnosis)
cm

accuracy <- sum(diag(cm)) / sum(cm)
accuracy
# 92% accuracy
```
Could try pruning the tree for even better accuracy

## Bagging 

```{r}
library(randomForest)

bag_model <- randomForest(
  diagnosis ~ ., 
  data = train,
  mtry = ncol(train) - 1,   # mtry = all predictors = bagging
  ntree = 500
)

bag_pred <- predict(bag_model, test)
mean(bag_pred == test$diagnosis)
```
96% accuracy with bagging- which is higher

## Random Forests
- should provide an improvement over bagged trees as it decorrelates them and reduces variance 

```{r}
rf_model <- randomForest(
  diagnosis ~ ., 
  data = train,
  mtry = sqrt(ncol(train) - 1), 
  ntree = 500,
  importance = TRUE
)

rf_pred <- predict(rf_model, test)
mean(rf_pred == test$diagnosis)

```
Gives accuracy of ~95%
